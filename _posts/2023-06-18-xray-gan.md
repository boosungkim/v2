---
layout: post
title: How GANs can be used to synthesize X-Rays
date: 2023-06-18 15:53:00-0400
description: How Generative Adversarial Networks are used to synthesize X-Rays
categories: computer-vision paper-review
giscus_comments: false
related_posts: true
toc:
  sidebar: right
---
[Synthesizing Chest X-Ray Pathology for Training
Deep Convolutional Neural Networks](https://www.researchgate.net/publication/328945795_Synthesizing_Chest_X-Ray_Pathology_for_Training_Deep_Convolutional_Neural_Networks)

My friend Daesung Kim, who studies the use of Computer Vision in medical research, recently approached me with an interesting paper to get my opinion. The paper was "Synthesizing Chest X-Ray Pathology for Training
Deep Convolutional Neural Networks," and it provided an insight on how GANs can be effectively be used in the medical world.

The authors of the paper suggest using Deep Convolutional Generative Adversarial Networks (DCGAN) to create synthesized chest X-rays through a custom model. The results show that the addition of these synthesized data leads to improved performance of pathology classification.

## What are GANs?
<img src="/assets/img/blogs/2023/2023-06-18-xray-gan/gan-example.png"  width="500">

*Figure 1: GAN generated art by u/TheAngryGoat*

GAN, or Generative Adversarial Network, is an area of Computer Vision that focuses on generating new images. The "Adversarial" part comes from the use of a generator and a discriminator components in the model. GANs are primarily responsible for the AI artworks you probably have seen floating around on the internet.

However, in many cases, GANs can be used to generate synthetic data samples, especially for when the training dataset is lacking.

<img src="/assets/img/blogs/2023/2023-06-18-xray-gan/gan-architecture.png"  width="600">

*Figure 2: GAN architectures*

The generator component is responsible for creating synthetic data samples. It takes random noise or input data as an initial input and generates new samples based on the patterns it has learned during training.

The discriminator component, on the other hand, acts as a classifier for the images. It aims to learn to distinguish between real data samples and synthetic samples created by the generator. The discriminator's objective is to correctly classify real and synthetic data samples.

During training, the generator and discriminator are pitted against each other in a game-like setting. The generator tries to generate synthetic samples that fool the discriminator into thinking they are real, while the discriminator tries to accurately classify the real and synthetic samples. This competition between the generator and discriminator helps both components improve over time.

As the training progresses, the generator becomes more adept at generating realistic samples that resemble the training data, while the discriminator becomes more accurate in distinguishing between real and synthetic samples. The ultimate goal is to train a generator that can produce synthetic data samples that are indistinguishable from real data, as judged by the discriminator.

## Synthesizing Chest X-Rays
<img src="/assets/img/blogs/2023/2023-06-18-xray-gan/synthesizing-xray-architecture.png"  width="600">

*Figure 3: Custom GAN architecture for synthesizing X-rays*

Daesung is tackling an imbalance issue in his X-ray training dataset at work, and he has turned to GANs to generate synthetic data samples.

In the paper, the authors present their custom DCGAN model, which utilizes fractionally-strided convolutions. They demonstrate that with each iteration, the model improves significantly, transforming distinguishable chest structures into detailed representations of ribs and vessels in just 18 epochs.

<img src="/assets/img/blogs/2023/2023-06-18-xray-gan/chest-gan-epochs.png"  width="600">

*Figure 4: Visualization of random samples for different epochs*

## Latent feature spaces
To efficiently measure the quality of the synthesized images, the authors utilize the concept of the Latent Feature Space.

Instead of measuing quality by pixel by pixel, the authors use a custom convolutional autoencoder (Figure 5), consisting of non-linear encoder function ($$g_e(x)$$), a non-linear decoder function ($$g_d(\psi)$$), 8 feature maps, to reduce the images of 256x256 to 16x16. The purpose of this autoencoder is to learn

$$g_d(g_e(X)) \approx X.$$

![image](/assets/img/blogs/2023/2023-06-18-xray-gan/autoencoder.png)

*Figure 5: Convolutional autoencoder*

To clarify, the latent space refers to the lower-dimensional representation of the synthesized image obtained through the autoencoder.

Using the autoencoder, the authors calculate a feature map in the latent space as a vector:

$$\psi_{c,v}^K = [\psi_{K,1}, \psi_{K,2}, \dots, \psi_{K,256}],$$

where $$c$$ represents the class of the image, $$v$$ represents the number of randomly selected X-ray images per class, $$K$$ indicates the categories of real and synthesized images.

The authors further compute the centroid of the latent space:

$$\hat{\gamma}_c^K = \frac{1}{N}\sum^N_{v=1}\hat{\psi}_{c,v}^K.$$

By comparing the centroids of the real and synthesized features, the authors demonstrate the similarity between the two.

## Why box plots?
![image](/assets/img/blogs/2023/2023-06-18-xray-gan/box-plots.png)

This is the part where we are a little stuck. These are box-plots of euclidean distances between centroid of each synthesized chest X-rays with the centroid of other real X-rays. Hence, it makes that only each class is the closest to real images (Synthesized closest box-plot lowest in (a), Synthesized Cardiomegaly lowest in (b), and so on).

The question is, should there not be just one centroid per class? Why is there a box plot for each class? The result from the earlier equation should return us a vector of a single centroid for the input feature vector. Then why are multiple centroids per class as shown in this box plot?

Our theory is that there are multiple centroids per class per real synthesized due to mini-batches (remember we took N random samples to get the feature vector earlier). We are still looking into this and will update the blog once we get a definitive answer. If you feel like you understand, please go ahead and let us know.

## Performance and Future directions
Finally, the model performance.

<img src="/assets/img/blogs/2023/2023-06-18-xray-gan/performance.png"  width="600">

|Dataset Name | Abbreviation | Description |
| :--- | :---: | :--- |
| Balanced Real Dataset | BR | Balanced version of original dataset |
| Balanced real dataset augmented with synthesized X-rays | BG | Balanced version of original dataset with synthesized data |
| Imbalanced real dataset | IR | All available original data |
| Imbalanced real dataset augmented with rotation and scaling | RS | All available original data with scaled and rotated variants |
| Imbalanced real dataset augmented with synthesized X-rays | GS | All original data with synthesized date (double the original amount) |

The results show that doubling the amount of data with DCGAN synthesized images (GS) improve the performance over the original (IR) for all models, and even improve ResNet50 by nearly 20%.

The results clearly indicate the success of synthesizing chest X-ray images with DCGANs.

## References
1. [Synthesizing Chest X-Ray Pathology for Training
Deep Convolutional Neural Networks](https://www.researchgate.net/publication/328945795_Synthesizing_Chest_X-Ray_Pathology_for_Training_Deep_Convolutional_Neural_Networks)
2. [(Figure 1) GAN art by u/theAngryGoat](https://www.reddit.com/r/StableDiffusion/comments/11bvx8d/using_controlnet_to_demonstrate_how_ai_generated/)