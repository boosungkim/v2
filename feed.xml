<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.3.2">Jekyll</generator><link href="https://boosungkim.com/feed.xml" rel="self" type="application/atom+xml" /><link href="https://boosungkim.com/" rel="alternate" type="text/html" hreflang="en" /><updated>2023-07-21T12:16:29+00:00</updated><id>https://boosungkim.com/feed.xml</id><title type="html">blank</title><subtitle>A lifelong learner interested in the modern world.
</subtitle><entry><title type="html">I got to meet Professor Andrew Ng! (SNU Data Science Day Recap)</title><link href="https://boosungkim.com/blog/2023/meeting-professor-andrew-ng/" rel="alternate" type="text/html" title="I got to meet Professor Andrew Ng! (SNU Data Science Day Recap)" /><published>2023-07-20T22:31:51+00:00</published><updated>2023-07-20T22:31:51+00:00</updated><id>https://boosungkim.com/blog/2023/meeting-professor-andrew-ng</id><content type="html" xml:base="https://boosungkim.com/blog/2023/meeting-professor-andrew-ng/"><![CDATA[<p><img src="/assets/img/blogs/2023/2023-07-20-meeting-professor-andrew-ng/champ.jpeg" width="500" /></p>

<p>Today, I had the opportunity to meet Professor Andrew Ng at Seoul National University’s Data Science Day!</p>

<p>I attended SNU’s Data Science Day, which introduced the new CHAMP AI center and hosted Professor Ng’s talk on AI trends and startups. I had to wake up at the break of dawn to attend, as I do not live in Seoul, but it was totally worth it!</p>

<h2 id="introducing-snus-champ">Introducing SNU’s CHAMP</h2>
<p><img src="/assets/img/blogs/2023/2023-07-20-meeting-professor-andrew-ng/birth-of-charm.JPG" width="500" /></p>

<p>I would like to start off the blog by first talking about the new SNU AI center, CHAMP (Center for optimizing Hyperscale AI Models and Platform), led by Professor Lee.</p>

<p>SNU established CHAMP to create Korean Hyperscale AI models in order to stay up to date with recent AI research and products. The goals for the next few years is to create hyperscale models similar to ChatGPT by collaborating with different research labs and companies in Korea.</p>

<p>I think this is a great opportunity for Korea to contribute to the world of AI, and I look forward to seeing future works from CHAMP. Who knows, perhaps I could apply to work there someday if such an opportunity arises.</p>

<h2 id="professor-ngs-talk">Professor Ng’s Talk</h2>
<p>Professor Ng’s talk was divided in to two parts: AI trends and AI startup process.</p>

<h3 id="new-electricity-and-the-decade-of-gan">New Electricity and the Decade of GAN</h3>
<p><img src="/assets/img/blogs/2023/2023-07-20-meeting-professor-andrew-ng/gan.JPG" width="500" /></p>

<p>Professor Ng kicked things off by repeating his favorite phrase: “AI is the new electricity.” He was kind enough to incorporate new Korean food he tried recently into his presentation!</p>

<p>Professor Ng claimed that if 2010s were the decade of supervised learning (with the boom of image recognition/detection), 2020s will be the boom of Generative AI. We already have ChatGPT and Midjourney, along with many other similar projects. Professor Ng predicts at least twice the amount of GAN progress to be made in the next few years, while Supervised Learning continues to grow as well.</p>

<h3 id="building-ai-businesses">Building AI Businesses</h3>
<p><img src="/assets/img/blogs/2023/2023-07-20-meeting-professor-andrew-ng/startup-process.JPG" width="500" /></p>

<p>Professor Ng firmly believes that AI technologies are general purpose technologies that can be useful for many tasks in different industries. It is not a magic tool that can be used to solve everything, but a major tool that can create opportunities to solve previously difficult problems.</p>

<p>Professor Ng, however, emphasized the importance on building powerful innovative products rather than surface level products.</p>

<p>For instance, when the iPhone first came out, there was an influx of apps on the new technology. There were surface level apps, like apps that converted the iPhone into a flashlight. While interesting, there was no true depth to the app, as it basically just had the camera flash turned on for an extended period of time. On the other hand, there were new apps that changed the world, such as Tinder, Maps, and iTunes.</p>

<p>Professor Ng spent the final portion of his presentation discussing his process for building AI startups, as can be seen in the photo above. It was very interesting hearing about the startup process, especially on how much time it takes before actually building the full product. Quite different building small personal projects.</p>

<p>Perhaps I could try reaching out to local companies or small startups and try to replicate this process myself to familiarize myself with the AI tools and skills. Something to sleep on.</p>

<h2 id="qa-and-selfies">Q&amp;A and Selfies</h2>
<p><img src="/assets/img/blogs/2023/2023-07-20-meeting-professor-andrew-ng/outside.JPG" width="400" /></p>

<p>There was a short Q&amp;A session, and I lined up behind the mic. I was a little scared if my question is good enough, but I’ve been working on stepping outside my comfort zone recently, so I went anyways. However, due to time constraints, I didn’t get to ask my question! Sadly, I will have to find some other opportunity to ask him in the future.</p>

<p>Although Professor Ng had another schedule right after, he still spent some time taking selfies with students while heading out. It was chaotic like a celebrity arriving at an airport, but that’s not surprising, considering how Professor Andrew Ng is like the Leonardo DiCaprio of AI. I might not have been able to ask him my question, but I luckily got to take a quick selfie with the legend.</p>

<h2 id="closing-words">Closing Words</h2>
<p><img src="/assets/img/blogs/2023/2023-07-20-meeting-professor-andrew-ng/ticket.JPG" width="300" /></p>

<p>Overall, meeting Professor Andrew Ng and speaking with different students and professors were very motivating. Professor Andrew Ng was incredibly smart, engaging, and very friendly. I will continue to work hard and create more opportunities to connect with others in the field.</p>

<p>Additionally, it’s been a while since I’ve posted new blogs. I have been spending my time learning tech stacks, wrapping up my internship at a local startup, and enjoying my final few weeks in Korea meeting friends and experts in CS.</p>]]></content><author><name></name></author><category term="story" /><summary type="html"><![CDATA[I got to meet Professor Andrew Ng for the first time at Seoul National University!]]></summary></entry><entry><title type="html">Introducing DeepLearnLab</title><link href="https://boosungkim.com/blog/2023/deeplearnlab/" rel="alternate" type="text/html" title="Introducing DeepLearnLab" /><published>2023-06-23T19:53:00+00:00</published><updated>2023-06-23T19:53:00+00:00</updated><id>https://boosungkim.com/blog/2023/deeplearnlab</id><content type="html" xml:base="https://boosungkim.com/blog/2023/deeplearnlab/"><![CDATA[<p><a href="https://boosungkim.com/dllab">DeepLearnLab</a></p>

<p>I am introducing my newest side project, DeepLearnLab, which is a Jekyll website with side-by-side notes and code implementations of popular Deep Learning papers and tools.</p>

<p><img src="/assets/img/blogs/2023/2023-06-23-deeplearnlab/dllab.jpg" width="500" /></p>

<p>Inspired by annotated deep learning papers on GitHub, I coded this website using Jekyll, Markdown, and Python to share my implemented code.</p>

<p>Over the past few weeks, I’ve implemented various models from Computer Vision, such as VGG, ResNet, and EfficientNet. All of the code, along with my notes from the papers, are now available at DeepLearnLab.</p>

<p>All my Deep Learning studies and paper code will be uploaded on DeeplearnLab from now on. I am currently working on a small-scale implementation of PyTorch’s autograd.</p>

<p>Go check it out, and feel free to leave a star on GitHub or contribute!</p>]]></content><author><name></name></author><category term="project" /><summary type="html"><![CDATA[Introducing DeepLearnLab, my new website with side-by-side notes and code implementations of all things DL]]></summary></entry><entry><title type="html">How GANs can be used to synthesize X-Rays</title><link href="https://boosungkim.com/blog/2023/xray-gan/" rel="alternate" type="text/html" title="How GANs can be used to synthesize X-Rays" /><published>2023-06-18T19:53:00+00:00</published><updated>2023-06-18T19:53:00+00:00</updated><id>https://boosungkim.com/blog/2023/xray-gan</id><content type="html" xml:base="https://boosungkim.com/blog/2023/xray-gan/"><![CDATA[<p><a href="https://www.researchgate.net/publication/328945795_Synthesizing_Chest_X-Ray_Pathology_for_Training_Deep_Convolutional_Neural_Networks">Synthesizing Chest X-Ray Pathology for Training
Deep Convolutional Neural Networks</a></p>

<p>My friend Daesung Kim, who studies the use of Computer Vision in medical research, recently approached me with an interesting paper to get my opinion. The paper was “Synthesizing Chest X-Ray Pathology for Training
Deep Convolutional Neural Networks,” and it provided an insight on how GANs can be effectively be used in the medical world.</p>

<p>The authors of the paper suggest using Deep Convolutional Generative Adversarial Networks (DCGAN) to create synthesized chest X-rays through a custom model. The results show that the addition of these synthesized data leads to improved performance of pathology classification.</p>

<h2 id="what-are-gans">What are GANs?</h2>
<p><img src="/assets/img/blogs/2023/2023-06-18-xray-gan/gan-example.png" width="500" /></p>

<p><em>Figure 1: GAN generated art by u/TheAngryGoat</em></p>

<p>GAN, or Generative Adversarial Network, is an area of Computer Vision that focuses on generating new images. The “Adversarial” part comes from the use of a generator and a discriminator components in the model. GANs are primarily responsible for the AI artworks you probably have seen floating around on the internet.</p>

<p>However, in many cases, GANs can be used to generate synthetic data samples, especially for when the training dataset is lacking.</p>

<p><img src="/assets/img/blogs/2023/2023-06-18-xray-gan/gan-architecture.png" width="600" /></p>

<p><em>Figure 2: GAN architectures</em></p>

<p>The generator component is responsible for creating synthetic data samples. It takes random noise or input data as an initial input and generates new samples based on the patterns it has learned during training.</p>

<p>The discriminator component, on the other hand, acts as a classifier for the images. It aims to learn to distinguish between real data samples and synthetic samples created by the generator. The discriminator’s objective is to correctly classify real and synthetic data samples.</p>

<p>During training, the generator and discriminator are pitted against each other in a game-like setting. The generator tries to generate synthetic samples that fool the discriminator into thinking they are real, while the discriminator tries to accurately classify the real and synthetic samples. This competition between the generator and discriminator helps both components improve over time.</p>

<p>As the training progresses, the generator becomes more adept at generating realistic samples that resemble the training data, while the discriminator becomes more accurate in distinguishing between real and synthetic samples. The ultimate goal is to train a generator that can produce synthetic data samples that are indistinguishable from real data, as judged by the discriminator.</p>

<h2 id="synthesizing-chest-x-rays">Synthesizing Chest X-Rays</h2>
<p><img src="/assets/img/blogs/2023/2023-06-18-xray-gan/synthesizing-xray-architecture.png" width="600" /></p>

<p><em>Figure 3: Custom GAN architecture for synthesizing X-rays</em></p>

<p>Daesung is tackling an imbalance issue in his X-ray training dataset at work, and he has turned to GANs to generate synthetic data samples.</p>

<p>In the paper, the authors present their custom DCGAN model, which utilizes fractionally-strided convolutions. They demonstrate that with each iteration, the model improves significantly, transforming distinguishable chest structures into detailed representations of ribs and vessels in just 18 epochs.</p>

<p><img src="/assets/img/blogs/2023/2023-06-18-xray-gan/chest-gan-epochs.png" width="600" /></p>

<p><em>Figure 4: Visualization of random samples for different epochs</em></p>

<h2 id="latent-feature-spaces">Latent feature spaces</h2>
<p>To efficiently measure the quality of the synthesized images, the authors utilize the concept of the Latent Feature Space.</p>

<p>Instead of measuing quality by pixel by pixel, the authors use a custom convolutional autoencoder (Figure 5), consisting of non-linear encoder function (\(g_e(x)\)), a non-linear decoder function (\(g_d(\psi)\)), 8 feature maps, to reduce the images of 256x256 to 16x16. The purpose of this autoencoder is to learn</p>

\[g_d(g_e(X)) \approx X.\]

<p><img src="/assets/img/blogs/2023/2023-06-18-xray-gan/autoencoder.png" alt="image" /></p>

<p><em>Figure 5: Convolutional autoencoder</em></p>

<p>To clarify, the latent space refers to the lower-dimensional representation of the synthesized image obtained through the autoencoder.</p>

<p>Using the autoencoder, the authors calculate a feature map in the latent space as a vector:</p>

\[\psi_{c,v}^K = [\psi_{K,1}, \psi_{K,2}, \dots, \psi_{K,256}],\]

<p>where \(c\) represents the class of the image, \(v\) represents the number of randomly selected X-ray images per class, \(K\) indicates the categories of real and synthesized images.</p>

<p>The authors further compute the centroid of the latent space:</p>

\[\hat{\gamma}_c^K = \frac{1}{N}\sum^N_{v=1}\hat{\psi}_{c,v}^K.\]

<p>By comparing the centroids of the real and synthesized features, the authors demonstrate the similarity between the two.</p>

<h2 id="why-box-plots">Why box plots?</h2>
<p><img src="/assets/img/blogs/2023/2023-06-18-xray-gan/box-plots.png" alt="image" /></p>

<p>This is the part where we are a little stuck. These are box-plots of euclidean distances between centroid of each synthesized chest X-rays with the centroid of other real X-rays. Hence, it makes that only each class is the closest to real images (Synthesized closest box-plot lowest in (a), Synthesized Cardiomegaly lowest in (b), and so on).</p>

<p>The question is, should there not be just one centroid per class? Why is there a box plot for each class? The result from the earlier equation should return us a vector of a single centroid for the input feature vector. Then why are multiple centroids per class as shown in this box plot?</p>

<p>Our theory is that there are multiple centroids per class per real synthesized due to mini-batches (remember we took N random samples to get the feature vector earlier). We are still looking into this and will update the blog once we get a definitive answer. If you feel like you understand, please go ahead and let us know.</p>

<h2 id="performance-and-future-directions">Performance and Future directions</h2>
<p>Finally, the model performance.</p>

<p><img src="/assets/img/blogs/2023/2023-06-18-xray-gan/performance.png" width="600" /></p>

<table>
  <thead>
    <tr>
      <th style="text-align: left">Dataset Name</th>
      <th style="text-align: center">Abbreviation</th>
      <th style="text-align: left">Description</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: left">Balanced Real Dataset</td>
      <td style="text-align: center">BR</td>
      <td style="text-align: left">Balanced version of original dataset</td>
    </tr>
    <tr>
      <td style="text-align: left">Balanced real dataset augmented with synthesized X-rays</td>
      <td style="text-align: center">BG</td>
      <td style="text-align: left">Balanced version of original dataset with synthesized data</td>
    </tr>
    <tr>
      <td style="text-align: left">Imbalanced real dataset</td>
      <td style="text-align: center">IR</td>
      <td style="text-align: left">All available original data</td>
    </tr>
    <tr>
      <td style="text-align: left">Imbalanced real dataset augmented with rotation and scaling</td>
      <td style="text-align: center">RS</td>
      <td style="text-align: left">All available original data with scaled and rotated variants</td>
    </tr>
    <tr>
      <td style="text-align: left">Imbalanced real dataset augmented with synthesized X-rays</td>
      <td style="text-align: center">GS</td>
      <td style="text-align: left">All original data with synthesized date (double the original amount)</td>
    </tr>
  </tbody>
</table>

<p>The results show that doubling the amount of data with DCGAN synthesized images (GS) improve the performance over the original (IR) for all models, and even improve ResNet50 by nearly 20%.</p>

<p>The results clearly indicate the success of synthesizing chest X-ray images with DCGANs.</p>

<h2 id="references">References</h2>
<ol>
  <li><a href="https://www.researchgate.net/publication/328945795_Synthesizing_Chest_X-Ray_Pathology_for_Training_Deep_Convolutional_Neural_Networks">Synthesizing Chest X-Ray Pathology for Training
Deep Convolutional Neural Networks</a></li>
  <li><a href="https://www.reddit.com/r/StableDiffusion/comments/11bvx8d/using_controlnet_to_demonstrate_how_ai_generated/">(Figure 1) GAN art by u/theAngryGoat</a></li>
</ol>]]></content><author><name></name></author><category term="computer-vision" /><category term="paper-review" /><summary type="html"><![CDATA[How Generative Adversarial Networks are used to synthesize X-Rays]]></summary></entry><entry><title type="html">The time I coded Chess with pen and paper in the Korean Army</title><link href="https://boosungkim.com/blog/2023/chess-engine-comeback/" rel="alternate" type="text/html" title="The time I coded Chess with pen and paper in the Korean Army" /><published>2023-06-17T19:53:00+00:00</published><updated>2023-06-17T19:53:00+00:00</updated><id>https://boosungkim.com/blog/2023/chess-engine-comeback</id><content type="html" xml:base="https://boosungkim.com/blog/2023/chess-engine-comeback/"><![CDATA[<p>This is a story about how I brought back and used my Custom Chess Engine during my service in the Korea Army. With limited resources and immense determination, I overcame obstacles to bring the joy of chess to fellow soldiers. Who knows, perhaps my creation still thrives today?</p>

<p><img src="/assets/img/blogs/2023/2023-06-17-chess-engine-comeback/army-time.jpeg" width="600" /></p>

<h2 id="my-military-service">My military service</h2>
<p>As many of you probably know, the military service is mandatory for most men in Korea. I enlisted in October 2021 and finished my service as a Sergeant this April.</p>

<p>As a mandatory duty, I enlisted in the Korean Army in October 2021 and completed my service as a Sergeant in April of this year. Assigned as a Data Analyst for the Tunnel Neutralization Team in the Demilitarized Zone (DMZ), I engaged in data processing and DMZ missions throughout my tenure.</p>

<p>Our base, situated outside the civilian zone, imposed numerous restrictions. Notably, we lacked modern facilities, including a PC room typically found in other bases.</p>

<h2 id="the-spark-of-an-idea">The spark of an idea</h2>
<p>Playing chess on military work computers was strictly prohibited due to security concerns. However, there were old recreational computers available for soldiers’ use during downtime—albeit without internet access.</p>

<p>In the evening, we were allowed 2-3 hours with our phones (on non-surveillance shifts). While soldiers utilized this time as they pleased, it was a fraction of the day, limiting options for entertainment. One day, upon learning of my background in Computer Science, a superior casually suggested creating games for these recreational computers.</p>

<p>That moment sparked my idea. I had previously developed a mostly finished Python-Chess project. Why not transfer the code to a military computer? Thus, I devoted the first half of 2022 to migrating my Chess project to a military machine.</p>

<p><img src="/assets/img/blogs/2023/2023-06-17-chess-engine-comeback/chess-github.png" width="600" /></p>

<h2 id="the-mighty-pen">The mighty pen</h2>
<p>Without internet access on the recreational computers, I couldn’t simply download the files from GitHub. However, I still had access to my phone during the evenings.</p>

<p>Initially, I intended to bring my phone into the recreational room and copy the code directly onto the computers. However, I discovered that phones were not allowed there due to the proximity of a [REDACTED] area.</p>

<p>Can you guess what I did? With approximately 2000 lines of code in my Chess project, I spent an hour or two every day for three months painstakingly copying code from GitHub onto paper and then transcribing it onto the recreational computers. Over 2000 lines of code.</p>

<p>On days without evening work shifts, I would receive my phone, find a hilltop bench with a serene river view, and meticulously transcribe the code, line by line, word by word.</p>

<p>There were moments when I accidentally wrote down the wrong line, resulting in hours of debugging. There were also days filled with emergency situations, like the threats of nuclear experimentation from North Korea last year. Despite the challenges, I persisted with unwavering determination.</p>

<p>After months of resolute work, the transfer was complete, and the recreational army computer officially hosted the Chess game.</p>

<h2 id="entertaining-a-whole-base">Entertaining a whole base</h2>
<p><img src="/assets/img/blogs/2023/2023-06-17-chess-engine-comeback/pc-room-army.jpg" width="600" /></p>

<p>Transferring 2000 lines of code was no small feat, but witnessing soldiers play my game to unwind after arduous work made it all worthwhile.</p>

<p>While I relish the problem-solving aspect of software development, the joy is multiplied when others actively use and enjoy my creations. Based on my rough estimate during service, I believe dozens of people played my game at least once.</p>

<p>Since my honorable discharge this year, I haven’t received much news about my former base. However, I like to imagine that future generations of soldiers will continue to relish the fruits of my labor.</p>

<h2 id="what-i-learned">What I learned</h2>
<h3 id="1-listen-to-what-people-want">1. Listen to what people want</h3>

<p>While it’s fulfilling to pursue projects that personally resonate, the true essence of product development lies in satisfying the desires of consumers. I’m grateful for recognizing the lack of daytime recreational activities based on my supervisor’s passing remark. Despite his unsuspecting nature, I employed my coding expertise to identify and resolve a problem.</p>

<h3 id="2-resistance-and-resilience">2. Resistance and Resilience</h3>

<p>This “code transfer” was far from the most complex task in my short CS career. However, the hurdles I encountered along the way made it one of the most challenging endeavors.</p>

<p>Coding within a controlled school environment is relatively straightforward, with minimal resistance. Yet, coding in the army exposed me to real-world obstacles. From the absence of internet and time constraints to skeptical commanders questioning my notebook filled with pages of code, I felt like I was navigating one fiery hoop after another.</p>

<p>I would imagine that coding outside of school is generally like this. During these hurdles, it is important to stay resilient and keep moving forward, one step at a time.</p>

<h3 id="3-better-code">3. Better code</h3>

<p>After revisiting my project code months after its initial creation, I identified repeated code sections requiring refactoring. Thanks to this project, I developed the habit of code reviewing, which I believe will prove invaluable in the future.</p>

<h3 id="4-speeding-things-up">4. Speeding things up</h3>

<p>Just before departing the base, a fellow soldier thanked me for my project and suggested speeding up the game as the Chess AI took a bit too long to make decisions. Originally a Python Chess project, I viewed this as an opportunity to learn C++ and optimize my code for increased speed.</p>

<p>I hope you enjoyed my story of coding with pen and paper for months in the Korean army, bringing the joy of Chess to fellow soldiers.</p>]]></content><author><name></name></author><category term="software" /><category term="story" /><summary type="html"><![CDATA[The story of how I coded Chess on pen and paper in the Korean Army.]]></summary></entry><entry><title type="html">EfficientNet Implementation</title><link href="https://boosungkim.com/blog/2023/efficientnet-implementation/" rel="alternate" type="text/html" title="EfficientNet Implementation" /><published>2023-06-15T19:53:00+00:00</published><updated>2023-06-15T19:53:00+00:00</updated><id>https://boosungkim.com/blog/2023/efficientnet-implementation</id><content type="html" xml:base="https://boosungkim.com/blog/2023/efficientnet-implementation/"><![CDATA[<p>Link to <a href="https://arxiv.org/abs/1905.11946">paper</a>
Link to <a href="https://github.com/boosungkim/milestone-cnn-model-implementations">my code</a></p>

<p>Unlike previous models, EfficientNet does not introduce a novel architecture. Rather, it introduces the idea of compound scaling and a base model, EfficientNet, to test the new scaling method.</p>

<p>The EfficientNet utilizes inverted residual blocks from MobileNetV2, Squeeze-and-Efficient layers from SE-Net, and stochastic depth.</p>

<h2 id="scaling-and-balancing">Scaling and balancing</h2>
<p><img src="/assets/img/blogs/2023/2023-06-15-efficientnet-implementation/model-scaling.png" alt="image" /></p>

<p>The authors argue that while scaling up depth, width, image resolution are common techniques to improve the model performance, previous papers use arbitrary scaling.</p>

<p>Depth is the most common method of scaling models. The VGG paper introduced the importance of depth, while ResNet and Densenet helped resolve the issue of training degradation.</p>

<p>While we have not spent time on width scaling, shallow networks generally use width scaling to capture features while being easy to train.</p>

<p>Scaling resolution is uncommon, but some networks GPipe utilize this to perform better. Resolution scaling is essentially increasing the width and height of the input images.</p>

<p>The empirical results of the paper indicate that a balance among width/depth/resolution can be achieved through compound scaling, which scales all three by a constant factor.</p>

<h2 id="compound-scaling">Compound scaling</h2>
<p>The idea behind compound scaling is to uniformly scale the depth, width, and resolution of the network in a principled manner. The authors introduce a compound coefficient, denoted as \(\phi\), that controls the scaling factor for each dimension. By varying the value of \(\phi\), the network can be scaled up or down while maintaining a balance among depth, width, and resolution.</p>

<p>The compound scaling is achieved by applying a set of predefined scaling rules. These rules specify how the depth, width, and resolution should be scaled based on the compound coefficient \(\phi\). By following these rules, the network’s capacity is increased in a balanced way, ensuring that no individual dimension dominates the scaling process.</p>

<p>\(depth: d = \alpha^{\phi}\)<br />
\(width: w = \beta^{\phi}\)<br />
\(resolution: r = \gamma^{\phi}\)<br />
such that \(\alpha \cdot \beta^2 \cdot \gamma^2 \approx 2\) and \(\alpha \geq 1, \beta \geq 1, \gamma \geq 1\).</p>

<p>For EfficientNet-B0, the authors first fixed \(\phi = 1\) and performed a grid search for \(\alpha, \beta, \gamma\) based on the equations above. The results showed that the best values are \(\alpha = 1.2, \beta = 1.1, \gamma = 1.15\).</p>

<p>Then, by fixing the \(\alpha, \beta, \gamma\) values found above, the authors calculated new \(\phi\) for scaled up versions of the model (EfficientNet-B1 ~ B7).</p>

<p>The authors used this approach to minimize search cost, but it is technically possible to find the optimal \(\alpha, \beta, \gamma\) values using a larger model.</p>

<h2 id="inverted-residual-block-mobilenet-and-se-net">Inverted residual block (MobileNet) and SE-Net</h2>

<p><img src="/assets/img/blogs/2023/2023-06-15-efficientnet-implementation/efficientnet-b0.png" alt="image" /></p>

<p><em>Table 1: EfficientNet-B0 architecture</em></p>

<p>As you can see from the table above, the EfficientNet-B0 baseline network utilizes inverted residual blocks (MBConv) from MobileNets.</p>

<p>In a normal residual block, bottlenecks are used to reduce the number of parameters and improve efficiency. To summarize, the block has a wide -&gt; narrow -&gt; structure using 1 by 1 convolutions.</p>

<p>On the other hand, an inverted residual block uses a narrow -&gt; wide -&gt; narrow approach with 1 by 1 convolutions and 3 by 3 depthwise convolutions.</p>

<p>Depthwise convolutions use far less parameters, as only one channel is neccessary for the filters. A single channel filter is used to calculate convolutions layer by layer, meaning less parameters.</p>

<p>The authors also state that they use Squeeze-and-Excitation networks.</p>

<h2 id="stochastic-depth">Stochastic depth</h2>
<p>Stochastic depth is essentially dropout for layers. For each mini-batch, some residual layers are completely dropped and only the residual skip connections are passed along.</p>

<p>This allows the network to train with a shorter effective depth, reducing the risk of overfitting and promoting regularization. By randomly dropping layers, stochastic depth provides a form of regularization similar to dropout but specifically tailored for residual networks.</p>

<h2 id="silu">SiLU</h2>
<p>The authors of the paper use SiLU instead of ReLU for feature activation.</p>

<h2 id="conclusion">Conclusion</h2>
<p>The big mistake I made was implementing this paper before MobileNet, as EfficientNet builds upon MobileNetV2. On the bright side, implementing MobileNetV2 should be relatively easy now using the code for the inverted residual block.</p>

<p>This is definitely the most complex model that I coded to date, as it uses many previous concepts. I haven’t run this model on CIFAR10 yet, as I am still going through the code to see if I implemented everything correctly, but it was quite an experience jumping back and forth between papers and presentation videos to try and grasp the idea of EfficientNet.</p>

<p>Once I finalize the model and run it on a dataset, I will post another blog post.</p>

<h2 id="references">References:</h2>
<ol>
  <li>https://paperswithcode.com/method/inverted-residual-block</li>
</ol>]]></content><author><name></name></author><category term="computer-vision" /><category term="implementation" /><summary type="html"><![CDATA[Understanding and implementing EfficientNet]]></summary></entry><entry><title type="html">I implemented 5 CNN papers. Here’s what I learned.</title><link href="https://boosungkim.com/blog/2023/the-imagenet-5/" rel="alternate" type="text/html" title="I implemented 5 CNN papers. Here’s what I learned." /><published>2023-06-15T19:53:00+00:00</published><updated>2023-06-15T19:53:00+00:00</updated><id>https://boosungkim.com/blog/2023/the-imagenet-5</id><content type="html" xml:base="https://boosungkim.com/blog/2023/the-imagenet-5/"><![CDATA[<p><a href="https://github.com/boosungkim/milestone-cnn-model-implementations">The Imagenet 5</a></p>

<p><em>An accompanying Converge2Diverge YouTube video is in the works.</em></p>

<p>As of this post, I have implemented 5 milestone CNN papers (6 really, if you include my implementation of MobileNet’s inverted residual block). I thinking about calling this short series of implementations “The ImageNet 5” (since all these models are based on ImageNet).</p>

<p>From this short experience, I truly believe that implementing papers and utilizing the code for other projects is the best way to learn deep learning. I learnt a lot, which can be summarized in 5 key points:</p>

<h2 id="1-break-down-the-model">1. Break down the model</h2>
<p>When I first implemented the VGG network and Residual Networks, I coded everything into one huge chunk. If you look at the ealiest GitHub versions of my code, you will be able to find it.</p>

<p>While the code may run, it is significantly harder to debug or to come back to later. If you read my post on <a href="/_posts/2023-06-15-efficientnet-implementation.md">EfficientNet</a>, you saw how we had to reuse the SE-Net code.</p>

<p>Additionally, if you run <code class="language-plaintext highlighter-rouge">summary()</code> from torchinfo, you can see that the function groups layers together by blocks or sequences. Organizing the dozens of of layers in the model into <code class="language-plaintext highlighter-rouge">features</code>, <code class="language-plaintext highlighter-rouge">classifier</code>, or other categories can be immensely helpful in reading the summary later. The official Pytorch implementations of the models do this as well, and when you utilize transfer learning, you can set which blocks of code to training mode easily due to this classification.</p>

<p>Finally, breaking down the model to smaller parts and implementing them first can help your understanding as well. Instead of overwhelming yourself with implementing everything, focus on implementing separate parts of the model.</p>

<h2 id="2-you-need-to-know-the-background-knowledge">2. You need to know the background knowledge</h2>
<p>I particularly felt this when implementing EfficientNet. Trying to implement it without proper knowledge of MobileNetV2 (its basis) was a complete nightmare, and I had to reference many explanations and videos to even code a running model.</p>

<p>When reading a paper, it includes references to previous works it is built on. Don’t be stubborn and actually read the references as well.</p>

<h2 id="3-take-notes-on-the-paper">3. Take notes on the paper</h2>

<p><img src="/assets/img/blogs/2023/2023-06-15-the-imagenet-5/notes-densenet.jpeg" width="600" height="300" /></p>

<p><em>My notes on the DenseNet paper</em></p>

<p><img src="/assets/img/blogs/2023/2023-06-15-the-imagenet-5/notes-resnet.jpeg" width="600" /></p>

<p><em>My notes on the ResNet paper</em></p>

<p>As mentioned in a previous blog, the parameters and hyperparameters of a model is often embedded in several different places in the paper. It’s like going on an Easter egg hunt for these clues.</p>

<p>Thanks to my prior research experience, I knew how to read papers on a high level: read the abstract -&gt; conclusion -&gt; figures/tables -&gt; introduction -&gt; main content. But through these implementations, I’ve gotten better at hunting down these little details.</p>

<p>Taking notes and writing down these details are very important, especially when you are trying to read several papers at once and start getting overwhelmed.</p>

<h2 id="4-have-a-set-environment">4. Have a set environment</h2>
<p>When you are testing these models, it can be very time-consuming to create a whole new environment. It also helps to use the same dataset with the same augmentations to benchmark your models. I ended up reusing the data augmentation and training/testing codes for all my models.</p>

<h2 id="5-hyperparameter-tuning-takes-all-day">5. Hyperparameter tuning takes all day</h2>
<p>The models I implemented were made for ImageNet, not CIFAR10. Thus, I had to finetune the hyperparameters to get close to the results on the paper. Even then, I was not able to fully replicate the results.</p>

<p>These models get more complicated as time goes on. We aren’t dealing with MNIST or Cats vs Dogs anymore. Each time I tweak a hyperparameter, I have to let the model run for hundreds of epoch, which can take hours. I’ve learnt to let the model run while I take care of other work or even while I am sleeping. It is important to get more efficient in running these models.</p>

<h2 id="what-now">What now?</h2>
<h3 id="more-experimentation">More experimentation</h3>
<p>So far, all my implementations focused on just the coding part with a little bit of finetuning. If my results matched the general trend of the model, I moved onto the next model. That’s not what a true researcher does.</p>

<p>A true researcher would try to replicate the different experiments done in each paper. While I will continue with my studies in other domains, I plan on coming back to some of these papers and properly replicating the results. I need to learn to plot loss and accuracy and compare these models against each other under the exact same circumstances.</p>

<h3 id="newer-papers">Newer papers</h3>
<p>So far, I only worked with older papers, the latest one being published in 2020 (which is pretty recent I guess). These older papers have more articles and explanations online that I was able to reference when I got stuck.</p>

<p>For instance, I was able to read an online article explaining the concept of growth rates in DenseNet, which was how I figured out that growth rates are just the number of channels the convolutional step is supposed to output.</p>

<p>As my skills and knowledge improve, I hope to be able to understand newer state-of-the-art papers from top conferences. My current goal is to be able to read and implement a 2023 NeurIPS paper this winter break.</p>

<h3 id="other-domain-studies">Other domain studies</h3>
<p>There are so many other areas outside of just image classification. The closest areas being Object Segmentation and Object Detection. Not to mention there is a whole world of Natural Language Processing waiting as well.</p>

<p>Perhaps I could do a Segmentation 5?</p>

<p>On a final note, I would like to thank my friend Daesung (Dan) Kim for his continuous help and support so far.</p>]]></content><author><name></name></author><category term="computer-vision" /><category term="implementation" /><summary type="html"><![CDATA[5 things I learnt from implementing 5 Computer Vision papers]]></summary></entry><entry><title type="html">SE-Net Implementation</title><link href="https://boosungkim.com/blog/2023/senet-implementation/" rel="alternate" type="text/html" title="SE-Net Implementation" /><published>2023-06-05T19:53:00+00:00</published><updated>2023-06-05T19:53:00+00:00</updated><id>https://boosungkim.com/blog/2023/senet-implementation</id><content type="html" xml:base="https://boosungkim.com/blog/2023/senet-implementation/"><![CDATA[<p>Link to <a href="https://arxiv.org/pdf/1709.01507.pdf">paper</a>
Link to <a href="https://github.com/boosungkim/milestone-cnn-model-implementations">my code</a></p>

<p>Squeeze-and-Excitation Networks, SE-Nets for short, are actually convolutional blocks that can be added to other models, like ResNet or VGG.</p>

<p><img src="/assets/img/blogs/2023/2023-06-05-senet-implementation/senet.png" alt="image" />
<em>Figure 1: Squeeze and Excitation Block</em></p>

<p>The key problem that the authors of the paper wants to address is the problem of implicit and local channel dependencies. They do so by adding the Squeeze-and-Excitation block, which relays channel-wise information.</p>

<p>By incorporating the SE blocks into the model, the network can adaptively recalibrate its feature maps to capture more discriminative information, leading to improved performance.</p>

<h2 id="implicit-local-channel-information">Implicit local channel information</h2>
<p><img src="/assets/img/blogs/2023/2023-06-05-senet-implementation/convolution.png" alt="image" />
<em>Figure 2: A normal convolution</em></p>

<p>In a normal convolution like the image above, channel dependencies are implicitly included in the outputs of the convolutional layers. In other words, each layer calculates the convolution on all the channels of a local region every step.</p>

<p>Due to the localness of the convolutions, each channel in the output contains implicit channel embeddings tangled with local spatial correlations. To simplify further, each pixel in a channel contains the channel embeddings of the local region on the convolution was calculated on.</p>

<h2 id="the-se-net">The SE-Net</h2>
<p>The SE-Net sets out to resolve this issue by introducing an explicit mechanism to model channel-wise relationships through the “squeeze” and “excitation” layers.</p>

<h3 id="squeeze">Squeeze</h3>
<p>The network first “squeezes” the outputs of the previous convolutional layer into \(channel\times1\times1\) shape using Global Average Pool.</p>

<h3 id="excitation">Excitation</h3>
<p>The network then performs “excitation” by performing two Fully Connected (FC) layers. The first FC layer reduces the number of channels by applying a reduction ratio. This reduction helps in reducing the computational complexity of the SE block. The second FC layer then expands the number of channels back to the original number. These FC layers capture the channel dependencies and learn channel-wise relationships based on the aggregated information from the squeeze operation.</p>

<p>FYI, because the FC layers are “fully connected,” every node is connected with each other. This is how the network captures channel-wise relationships and dependencies.</p>

<h3 id="rescale">Rescale</h3>
<p>Finally, the SE-Net rescales the output back to the input dimensions by using the Unflatten operation and channel-wise multiplication. The channel-wise attention weights are then applied by element-wise multiplication, allowing the network to selectively amplify or suppress the channel activations based on their importance.</p>

<h2 id="se-net-implementation">SE-Net implementation</h2>
<h3 id="implementation">Implementation</h3>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">se_block</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">input_channels</span><span class="p">,</span> <span class="n">reduction_ratio</span><span class="p">):</span>
        <span class="nf">super</span><span class="p">(</span><span class="n">se_block</span><span class="p">,</span><span class="n">self</span><span class="p">).</span><span class="nf">__init__</span><span class="p">()</span>
        <span class="n">self</span><span class="p">.</span><span class="n">sequence</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Sequential</span><span class="p">(</span>
            <span class="c1"># 1. Squeeze
</span>            <span class="n">nn</span><span class="p">.</span><span class="nc">AdaptiveAvgPool2d</span><span class="p">((</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">)),</span> <span class="c1"># output: bxCx1x1
</span>            <span class="n">nn</span><span class="p">.</span><span class="nc">Flatten</span><span class="p">(),</span>  <span class="c1"># output: bxC
</span>            <span class="c1"># 2. Excitation
</span>            <span class="n">nn</span><span class="p">.</span><span class="nc">Linear</span><span class="p">(</span><span class="n">input_channels</span><span class="p">,</span> <span class="n">input_channels</span> <span class="o">//</span> <span class="n">reduction_ratio</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="bp">False</span><span class="p">),</span> <span class="c1"># output: bxC/r
</span>            <span class="n">nn</span><span class="p">.</span><span class="nc">ReLU</span><span class="p">(),</span> <span class="c1"># output: bxC/r
</span>            <span class="n">nn</span><span class="p">.</span><span class="nc">Linear</span><span class="p">(</span><span class="n">input_channels</span> <span class="o">//</span> <span class="n">reduction_ratio</span><span class="p">,</span> <span class="n">input_channels</span><span class="p">),</span> <span class="c1"># output: bxC
</span>            <span class="n">nn</span><span class="p">.</span><span class="nc">Sigmoid</span><span class="p">(),</span> <span class="c1"># output: bxC
</span>            <span class="n">nn</span><span class="p">.</span><span class="nc">Unflatten</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="p">(</span><span class="n">input_channels</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">))</span> <span class="c1"># output: bxCx1x1
</span>        <span class="p">)</span>
        <span class="k">pass</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="n">self</span><span class="p">,</span><span class="n">x</span><span class="p">):</span>
        <span class="n">z</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">sequence</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="c1"># 3. Rescale
</span>        <span class="n">z</span> <span class="o">=</span> <span class="n">x</span><span class="o">*</span><span class="n">z</span> <span class="c1"># output: bxCxHxW
</span>        <span class="k">return</span> <span class="n">z</span>
</code></pre></div></div>
<p>There is not much to add here, as the code follows the description one-to-one. One thing to add is that I originally used Pytorch’s AvgPool2d with manually calculated channel width and height, but Pytorch has the AdaptiveAvgPool2d which handles the dimensions for you.</p>

<h3 id="network-summary">Network summary</h3>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="o">&gt;&gt;&gt;</span> <span class="n">model</span> <span class="o">=</span> <span class="nf">se_block</span><span class="p">(</span><span class="mi">64</span><span class="p">,</span> <span class="mi">16</span><span class="p">)</span>
<span class="o">&gt;&gt;&gt;</span> <span class="nf">summary</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">input_size</span><span class="o">=</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">64</span><span class="p">,</span><span class="mi">32</span><span class="p">,</span><span class="mi">32</span><span class="p">),</span> <span class="n">col_names</span><span class="o">=</span><span class="p">[</span><span class="sh">"</span><span class="s">input_size</span><span class="sh">"</span><span class="p">,</span><span class="sh">"</span><span class="s">output_size</span><span class="sh">"</span><span class="p">,</span><span class="sh">"</span><span class="s">num_params</span><span class="sh">"</span><span class="p">])</span>
</code></pre></div></div>
<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>===================================================================================================================
Layer (type:depth-idx)                   Input Shape               Output Shape              Param #
===================================================================================================================
se_block                                 [1, 64, 32, 32]           [1, 64, 32, 32]           --
├─Sequential: 1-1                        [1, 64, 32, 32]           [1, 64, 1, 1]             --
│    └─AdaptiveAvgPool2d: 2-1            [1, 64, 32, 32]           [1, 64, 1, 1]             --
│    └─Flatten: 2-2                      [1, 64, 1, 1]             [1, 64]                   --
│    └─Linear: 2-3                       [1, 64]                   [1, 4]                    256
│    └─ReLU: 2-4                         [1, 4]                    [1, 4]                    --
│    └─Linear: 2-5                       [1, 4]                    [1, 64]                   320
│    └─Sigmoid: 2-6                      [1, 64]                   [1, 64]                   --
│    └─Unflatten: 2-7                    [1, 64]                   [1, 64, 1, 1]             --
===================================================================================================================
Total params: 576
Trainable params: 576
Non-trainable params: 0
Total mult-adds (Units.MEGABYTES): 0.00
===================================================================================================================
Input size (MB): 0.26
Forward/backward pass size (MB): 0.00
Params size (MB): 0.00
Estimated Total Size (MB): 0.27
===================================================================================================================
</code></pre></div></div>
<p>You may notice that the number of parameters is relatively low due to the use the reduction ratio. Thus, the inclusion of SE-Nets in other models will not significantly impact the number of total parameters.</p>

<h2 id="se-resnet">SE-ResNet</h2>
<p>With the SE-Net coded, it is trivial to add it to our previous models. I decided to test it on ResNet.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">SE_ResidualBlockBottleneck</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="n">expansion</span> <span class="o">=</span> <span class="mi">4</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">input_channels</span><span class="p">,</span> <span class="n">in_channels</span><span class="p">,</span> <span class="n">reduction_ratio</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="mi">1</span><span class="p">):</span>
        <span class="nf">super</span><span class="p">(</span><span class="n">SE_ResidualBlockBottleneck</span><span class="p">,</span> <span class="n">self</span><span class="p">).</span><span class="nf">__init__</span><span class="p">()</span>

        <span class="n">self</span><span class="p">.</span><span class="n">block</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Sequential</span><span class="p">(</span>
            <span class="n">nn</span><span class="p">.</span><span class="nc">Conv2d</span><span class="p">(</span><span class="n">in_channels</span><span class="o">=</span><span class="n">input_channels</span><span class="p">,</span> <span class="n">out_channels</span><span class="o">=</span><span class="n">in_channels</span><span class="p">,</span> <span class="n">kernel_size</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="bp">False</span><span class="p">),</span>
            <span class="n">nn</span><span class="p">.</span><span class="nc">BatchNorm2d</span><span class="p">(</span><span class="n">in_channels</span><span class="p">),</span>
            <span class="n">nn</span><span class="p">.</span><span class="nc">ReLU</span><span class="p">(),</span>
            <span class="n">nn</span><span class="p">.</span><span class="nc">Conv2d</span><span class="p">(</span><span class="n">in_channels</span><span class="o">=</span><span class="n">in_channels</span><span class="p">,</span> <span class="n">out_channels</span><span class="o">=</span><span class="n">in_channels</span><span class="p">,</span> <span class="n">kernel_size</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="n">stride</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="bp">False</span><span class="p">),</span>
            <span class="n">nn</span><span class="p">.</span><span class="nc">BatchNorm2d</span><span class="p">(</span><span class="n">in_channels</span><span class="p">),</span>
            <span class="n">nn</span><span class="p">.</span><span class="nc">ReLU</span><span class="p">(),</span>
            <span class="n">nn</span><span class="p">.</span><span class="nc">Conv2d</span><span class="p">(</span><span class="n">in_channels</span><span class="o">=</span><span class="n">in_channels</span><span class="p">,</span> <span class="n">out_channels</span><span class="o">=</span><span class="n">in_channels</span><span class="o">*</span><span class="mi">4</span><span class="p">,</span> <span class="n">kernel_size</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="bp">False</span><span class="p">),</span>
            <span class="n">nn</span><span class="p">.</span><span class="nc">BatchNorm2d</span><span class="p">(</span><span class="n">in_channels</span><span class="o">*</span><span class="mi">4</span><span class="p">)</span>
        <span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">relu</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">ReLU</span><span class="p">()</span>
        <span class="n">self</span><span class="p">.</span><span class="n">se_block</span> <span class="o">=</span> <span class="nc">SE_block</span><span class="p">(</span><span class="n">in_channels</span><span class="o">*</span><span class="mi">4</span><span class="p">,</span> <span class="n">reduction_ratio</span><span class="p">)</span>

        <span class="k">if</span> <span class="n">stride</span> <span class="o">!=</span> <span class="mi">1</span> <span class="ow">or</span> <span class="n">input_channels</span> <span class="o">!=</span> <span class="n">self</span><span class="p">.</span><span class="n">expansion</span><span class="o">*</span><span class="n">in_channels</span><span class="p">:</span>
            <span class="n">self</span><span class="p">.</span><span class="n">shortcut</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Sequential</span><span class="p">(</span>
                <span class="n">nn</span><span class="p">.</span><span class="nc">Conv2d</span><span class="p">(</span><span class="n">in_channels</span><span class="o">=</span><span class="n">input_channels</span><span class="p">,</span> <span class="n">out_channels</span><span class="o">=</span><span class="n">in_channels</span><span class="o">*</span><span class="mi">4</span><span class="p">,</span> <span class="n">kernel_size</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="n">stride</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="bp">False</span><span class="p">),</span>
                <span class="n">nn</span><span class="p">.</span><span class="nc">BatchNorm2d</span><span class="p">(</span><span class="n">in_channels</span><span class="o">*</span><span class="mi">4</span><span class="p">)</span>
            <span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">self</span><span class="p">.</span><span class="n">shortcut</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Sequential</span><span class="p">()</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="n">z</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">block</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">z</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">se_block</span><span class="p">(</span><span class="n">z</span><span class="p">)</span>
        <span class="n">z</span> <span class="o">+=</span> <span class="n">self</span><span class="p">.</span><span class="nf">shortcut</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">z</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">relu</span><span class="p">(</span><span class="n">z</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">z</span>
</code></pre></div></div>
<p>Notice that the only additions made are <code class="language-plaintext highlighter-rouge">self.se_block = SE_block(in_channels*4, reduction_ratio)</code> and <code class="language-plaintext highlighter-rouge">z = self.se_block(z)</code>. The number of channels is <code class="language-plaintext highlighter-rouge">in_channel*4</code> as that is the result of the residual block (bottleneck).</p>

<h3 id="experimentation">Experimentation</h3>
<p>When run under the same hyperparameters as ResNet, SE-ResNet produces a training accuracy of 95.7% and a testing accuracy of 88.6%. Not quite as a good as DenseNet, but I only ran SE-ResNet for about 400 epoch, significantly less than my runs for ResNet and DenseNet.</p>

<h2 id="conclusion">Conclusion</h2>
<p>The results fall in line with the results of the paper. The accuracies were increased and the network converged faster than the original.</p>

<p>While I only tested the SE-Network on ResNet, the squeeze-and-excitation method can be included in many other models as well.</p>

<h2 id="references">References:</h2>
<ol>
  <li>https://towardsdatascience.com/practical-graph-neural-networks-for-molecular-machine-learning-5e6dee7dc003</li>
</ol>]]></content><author><name></name></author><category term="computer-vision" /><category term="implementation" /><summary type="html"><![CDATA[Understanding and implementing SE-Networks]]></summary></entry><entry><title type="html">DenseNet Implementation</title><link href="https://boosungkim.com/blog/2023/densenet-implementation/" rel="alternate" type="text/html" title="DenseNet Implementation" /><published>2023-06-02T19:53:00+00:00</published><updated>2023-06-02T19:53:00+00:00</updated><id>https://boosungkim.com/blog/2023/densenet-implementation</id><content type="html" xml:base="https://boosungkim.com/blog/2023/densenet-implementation/"><![CDATA[<p>Link to <a href="https://arxiv.org/abs/1608.06993">paper</a>
Link to <a href="https://github.com/boosungkim/milestone-cnn-model-implementations">my code</a></p>

<p>DenseNets are basically ResNets but with residual connections between every single layer.</p>

<p><img src="/assets/img/blogs/2023/2023-06-02-densenet-implementation/densenet.png" width="600" />
<!-- ![image](/assets/img/blogs/2023-06-02-densenet-implementation/densenet.png) -->
<em>Figure 1: DenseNet</em></p>

<p>The idea is to concatenate the feature maps of all the preceding layers with the result of the current layer. The authors of “Densely Connected Convolutional Networks” argue that the element-wise summation used in Residual Networks may impede the flow of information.</p>

<p>By concatenating the feature maps, the authors improves the information flow as each layer would have direct information from all previous layers.</p>

\[x_l = H_l([x_0, x_1, \dots, x_{l-1}])\]

<p>What this equation basically means is that instead of each layer taking in only the previous output, like VGG, taking in the previous output and the output of the block before, like ResNet, each layer takes in the concatenation of all previous layers in the Dense block.</p>

<h2 id="growth-rate">Growth rate</h2>
<p>So what is the growth rate in DenseNets? The growth rate, as its name suggests, is the growth rate of the number of channels for each dense layer. For instance, if we start off with 64 layers and the growth rate is 32, the convolutional sequence in the first layer will create an output of 32 channels. The dense layer will conclude by concatenating the previous outputs, giving us 96 channels. The next layer will then have 128 layers, and so on.</p>

<p>The authors of the paper emperically prove that the dense network works sufficiently well with relatively small growth rates, like \(k=12\).</p>

<h2 id="bottleneck-layers-in-densenet">Bottleneck layers in DenseNet</h2>
<p>With the continuous increase in the number of channels, you may understand the importance of using bottlenecks if you read my previous blog post on <a href="/_posts/2023-06-01-resnet34-implementation.md">ResNet</a>.</p>

<p>To recap, bottleneck layers decrease the number of channels using 1 by 1 convolutions for more efficient computation.</p>

<p>By employing 1 by 1 convolutions, the bottleneck layers compress the information carried by the feature maps, reducing the computational load while still preserving essential features.</p>

<p>Unlike ResNet, DenseNets utilize just one 1 by 1 convolutions per layer to reduce the number of channels.</p>

<h2 id="transition-layers">Transition layers</h2>
<p>Bottleneck layers alone are not enough to improve the model compactness. This is where the authors introduce compression by Transition layers.</p>

<p>Essentially, the transition layer reduces the number of channels by a factor of \(\theta\), which the paper sets as 0.5 for DenseNet-C. Hence, DenseNet-C halves the number of channels every transition layer.</p>

<h2 id="implementation-detail">Implementation detail</h2>
<p>We now have the building blocks for DesneNet.</p>

<h3 id="dense-layer">Dense layer</h3>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">DenseLayer</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">input_channels</span><span class="p">,</span> <span class="n">growth_rate</span><span class="p">):</span>
        <span class="nf">super</span><span class="p">(</span><span class="n">DenseLayer</span><span class="p">,</span> <span class="n">self</span><span class="p">).</span><span class="nf">__init__</span><span class="p">()</span>
        <span class="n">self</span><span class="p">.</span><span class="n">growth_rate</span> <span class="o">=</span> <span class="n">growth_rate</span>

        <span class="n">self</span><span class="p">.</span><span class="n">layer</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Sequential</span><span class="p">(</span>
            <span class="n">nn</span><span class="p">.</span><span class="nc">BatchNorm2d</span><span class="p">(</span><span class="n">input_channels</span><span class="p">),</span>
            <span class="n">nn</span><span class="p">.</span><span class="nc">ReLU</span><span class="p">(),</span>
            <span class="n">nn</span><span class="p">.</span><span class="nc">Conv2d</span><span class="p">(</span><span class="n">in_channels</span><span class="o">=</span><span class="n">input_channels</span><span class="p">,</span> <span class="n">out_channels</span><span class="o">=</span><span class="mi">4</span><span class="o">*</span><span class="n">input_channels</span><span class="p">,</span> <span class="n">kernel_size</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="bp">False</span><span class="p">),</span>
            <span class="n">nn</span><span class="p">.</span><span class="nc">BatchNorm2d</span><span class="p">(</span><span class="mi">4</span><span class="o">*</span><span class="n">input_channels</span><span class="p">),</span>
            <span class="n">nn</span><span class="p">.</span><span class="nc">ReLU</span><span class="p">(),</span>
            <span class="n">nn</span><span class="p">.</span><span class="nc">Conv2d</span><span class="p">(</span><span class="n">in_channels</span><span class="o">=</span><span class="mi">4</span><span class="o">*</span><span class="n">input_channels</span><span class="p">,</span> <span class="n">out_channels</span><span class="o">=</span><span class="n">growth_rate</span><span class="p">,</span> <span class="n">kernel_size</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>
        <span class="p">)</span>
    
    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="n">self</span><span class="p">,</span><span class="n">x</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">self</span><span class="p">.</span><span class="nf">_forward_implementation</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
    
    <span class="k">def</span> <span class="nf">_forward_implementation</span><span class="p">(</span><span class="n">self</span><span class="p">,</span><span class="n">x</span><span class="p">):</span>
        <span class="n">z</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">layer</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">z</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">cat</span><span class="p">([</span><span class="n">z</span><span class="p">,</span> <span class="n">x</span><span class="p">],</span> <span class="mi">1</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">z</span>
</code></pre></div></div>
<p>The difference here from the Residual block code is that we use <code class="language-plaintext highlighter-rouge">torch.cat([z,x],1)</code> to concatenate the previous results. <code class="language-plaintext highlighter-rouge">z</code> is the result of the convolutions, which will return <code class="language-plaintext highlighter-rouge">growth_rate</code> number of channels. <code class="language-plaintext highlighter-rouge">x</code> is the result of the previous layer’s output.</p>

<h3 id="transition-layer">Transition layer</h3>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">TransitionBlock</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">input_channels</span><span class="p">):</span>
        <span class="nf">super</span><span class="p">(</span><span class="n">TransitionBlock</span><span class="p">,</span> <span class="n">self</span><span class="p">).</span><span class="nf">__init__</span><span class="p">()</span>
        <span class="n">self</span><span class="p">.</span><span class="n">input_channels</span> <span class="o">=</span> <span class="n">input_channels</span>
        <span class="n">self</span><span class="p">.</span><span class="n">output_channels</span> <span class="o">=</span> <span class="n">input_channels</span> <span class="o">//</span> <span class="mi">2</span>

        <span class="n">self</span><span class="p">.</span><span class="n">layer</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Sequential</span><span class="p">(</span>
            <span class="n">nn</span><span class="p">.</span><span class="nc">BatchNorm2d</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">input_channels</span><span class="p">),</span>
            <span class="n">nn</span><span class="p">.</span><span class="nc">Conv2d</span><span class="p">(</span><span class="n">in_channels</span><span class="o">=</span><span class="n">self</span><span class="p">.</span><span class="n">input_channels</span><span class="p">,</span> <span class="n">out_channels</span><span class="o">=</span><span class="n">self</span><span class="p">.</span><span class="n">output_channels</span><span class="p">,</span> <span class="n">kernel_size</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="bp">False</span><span class="p">),</span>
            <span class="n">nn</span><span class="p">.</span><span class="nc">AvgPool2d</span><span class="p">(</span><span class="n">kernel_size</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
        <span class="p">)</span>
    
    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="n">self</span><span class="p">,</span><span class="n">x</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">self</span><span class="p">.</span><span class="nf">layer</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
    
    <span class="k">def</span> <span class="nf">output_channels_num</span><span class="p">(</span><span class="n">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">self</span><span class="p">.</span><span class="n">output_channels</span>
</code></pre></div></div>
<p>Even simpler! We just use a combination of BatchNorm, 1 by 1 convolution, and AvgPool to reduce the number of channels by a factor of 0.5 and the width and height by 0.5.</p>

<p>I just set \(\theta\) to be 0.5 as the paper does, but you can adjust it to be some other value.</p>

<h2 id="conclusion">Conclusion</h2>
<p>Despite having a similar depth and the number of parameters as ResNet, my variation of DenseNet produced an improvement in both training and testing accuracies. My densenet produced a training accuracy of 96.6% and testing accuracy of 88.7%.</p>

<p>The higher training accuracy before plateuing proves that densely connected layers is an improvement over shortcut connections from ResNet. The direct information flow between layers in DenseNet through concatenation has proven to be beneficial for learning complex representations and improving model performance.</p>]]></content><author><name></name></author><category term="computer-vision" /><category term="implementation" /><summary type="html"><![CDATA[Understanding and implementing Dense Networks]]></summary></entry><entry><title type="html">ResNet34 Model Implementation</title><link href="https://boosungkim.com/blog/2023/resnet34-implementation/" rel="alternate" type="text/html" title="ResNet34 Model Implementation" /><published>2023-06-01T19:53:00+00:00</published><updated>2023-06-01T19:53:00+00:00</updated><id>https://boosungkim.com/blog/2023/resnet34-implementation</id><content type="html" xml:base="https://boosungkim.com/blog/2023/resnet34-implementation/"><![CDATA[<p>Link to <a href="https://arxiv.org/pdf/1512.03385.pdf">paper</a>
Link to <a href="https://github.com/boosungkim/milestone-cnn-model-implementations">my code</a></p>

<p>Continuing on from my previous <a href="./2023-05-31-first-paper-implementation.md">VGG implementation</a>, I worked on implementing ResNet next.</p>

<h2 id="what-is-resnet">What is ResNet?</h2>
<p>The researchers behind ‘Deep Residual Learning for Image Recognition’ aimed to address the common ‘degradation problem’ encountered in deep convolutional networks.</p>

<p>When the depth of a CNN model is increased, it initially shows an improvement in performance but eventually degrades rapidly as the training accuracy plateus or even worsens over time.</p>

<h3 id="the-common-misconception">The common misconception</h3>
<p>The common misconception is that this rapid degradation in accuracy is caused by overfitting.</p>

<p>While overfitting due to exploding/vanishing gradients is expected problems in very deep networks, it has been accounted for by nomalized initializations of the dataset and the intermediate Batch Normalization layers.</p>

<p>The degradation is definitely not caused by overfitting, as adding more layers actually causes the <em>training error</em> to <strong>increase</strong>.</p>

<p>So what causes this degradation problem? Even the researchers in the paper are not sure. Their conclusion is that “deep plain nets may have exponentially low convergence rates.”</p>

<h2 id="what-is-a-residual-learning">What is a Residual Learning?</h2>
<p>So if we don’t know what causes the degradation problem, do we at least know how to prevent it? That’s where residual mapping in Residual Learning come into play.</p>

<p><img src="/assets/img/blogs/2023/2023-06-01-resnet34-implementation/residual-block.jpeg" width="600" />
<em>Figure 1: A residual block</em></p>

\[H(x) := F(x) + x\]

<p>If we let \(x\) be the incoming feature, the \(F(x)\) is the normal weighted layers that CNNs have (Convolutional, Batch Normalization, ReLU layers). The original \(x\) is then added (element-wise addition) to \(F(x)\) to produce \(H(x)\).</p>

<p>Essentially, the original features are added to the result of the weighted layers, and this whole process is one residual block. The idea is that, in the worst case scenario where \(F(x)\) produce a useless tensor filled with \(0\)s, the identity will be added back in to pass on a useful feature to the next block. Shortcut connections can only help the network.</p>

<p>As this is a CNN model, downsampling is necessary. The issue is that the dimensions of \(F(x)\) and \(x\) would be different after downsampling. In such cases, the \(F(x)\) and \(W_1x\) are added together, where the square matrix \(W_1\) is used to match dimensions.</p>

<h2 id="what-is-a-bottleneck-residual-block">What is a Bottleneck Residual Block?</h2>
<p>A bottleneck residual block is a variant of the residual block that uses 1 by 1 convolutions to create a “bottleneck.” The primary purpose of a bottleneck is to reduce the number for parameters in the network.</p>

<p><img src="/assets/img/blogs/2023/2023-06-01-resnet34-implementation/residual-block-bottleneck.png" width="600" />
<em>Figure 2: A no-bottleneck residual block (left) vs a bottleneck residual block (right)</em></p>

<p>By utilizing a 1x1 convolution, the network first reduces the number of channels before applying the subsequent 3x3 convolution. The output is then restored to the original channel length by another 1x1 convolution.</p>

<p>The reduction in the number of channels leads to a significant reduction in the number of parameters in the network. This parameter reduction allows for more efficient training and enables the use of deeper and more complex architectures while managing computational resources effectively.</p>

<p>Bottleneck residual blocks have become a key component in deeper ResNet variants, contributing to their improved performance and efficiency.</p>

<h2 id="understanding-dimensionality-reduction">Understanding dimensionality reduction</h2>
<p>As this is a CNN model, downsampling is necessary. The issue is that the dimensions of \(F(x)\) and \(x\) would be different after downsampling. In such cases, the \(F(x)\) and \(W_1x\) are added together, where the square matrix \(W_1\) is used to match dimensions.</p>

<p>Below is for simple shortcut connections with elementwise addition.</p>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">z</span> <span class="o">=</span> <span class="n">z</span> <span class="o">+</span> <span class="n">identity</span>
</code></pre></div></div>

<p>But what about the dimensionality reductions? Well it’s just a sequence of a Conv2d layer and a BatchNorm2d layer, but matching the dimensions was a little confusing at first.</p>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">z</span> <span class="o">=</span> <span class="n">z</span> <span class="o">+</span> <span class="n">self</span><span class="p">.</span><span class="nf">dim_reduction</span><span class="p">(</span><span class="n">identity</span><span class="p">)</span>
<span class="bp">...</span>

<span class="k">def</span> <span class="nf">skip_connection</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">input_channels</span><span class="p">,</span> <span class="n">output_channels</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Sequential</span><span class="p">(</span>
            <span class="n">nn</span><span class="p">.</span><span class="nc">Conv2d</span><span class="p">(</span><span class="n">in_channels</span><span class="o">=</span><span class="n">input_channels</span><span class="p">,</span> <span class="n">out_channels</span><span class="o">=</span><span class="n">output_channels</span><span class="p">,</span> <span class="n">kernel_size</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="mi">0</span><span class="p">),</span>
            <span class="n">nn</span><span class="p">.</span><span class="nc">BatchNorm2d</span><span class="p">(</span><span class="n">output_channels</span><span class="p">)</span>
        <span class="p">)</span>
</code></pre></div></div>

<h2 id="implementation">Implementation</h2>
<h3 id="a-normal-residual-block">A normal residual block</h3>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">ResidualBlockNoBottleneck</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="n">expansion</span> <span class="o">=</span> <span class="mi">1</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">input_channels</span><span class="p">,</span> <span class="n">output_channels</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="mi">1</span><span class="p">):</span>
        <span class="nf">super</span><span class="p">(</span><span class="n">ResidualBlockNoBottleneck</span><span class="p">,</span> <span class="n">self</span><span class="p">).</span><span class="nf">__init__</span><span class="p">()</span>
        <span class="n">self</span><span class="p">.</span><span class="n">expansion</span> <span class="o">=</span> <span class="mi">1</span>

        <span class="n">self</span><span class="p">.</span><span class="n">block</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Sequential</span><span class="p">(</span>
            <span class="n">nn</span><span class="p">.</span><span class="nc">Conv2d</span><span class="p">(</span><span class="n">in_channels</span><span class="o">=</span><span class="n">input_channels</span><span class="p">,</span> <span class="n">out_channels</span><span class="o">=</span><span class="n">output_channels</span><span class="p">,</span> <span class="n">kernel_size</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="n">stride</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="bp">False</span><span class="p">),</span>
            <span class="n">nn</span><span class="p">.</span><span class="nc">BatchNorm2d</span><span class="p">(</span><span class="n">output_channels</span><span class="p">),</span>
            <span class="n">nn</span><span class="p">.</span><span class="nc">ReLU</span><span class="p">(),</span>
            <span class="n">nn</span><span class="p">.</span><span class="nc">Conv2d</span><span class="p">(</span><span class="n">in_channels</span><span class="o">=</span><span class="n">output_channels</span><span class="p">,</span> <span class="n">out_channels</span><span class="o">=</span><span class="n">output_channels</span><span class="p">,</span> <span class="n">kernel_size</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="bp">False</span><span class="p">),</span>
            <span class="n">nn</span><span class="p">.</span><span class="nc">BatchNorm2d</span><span class="p">(</span><span class="n">output_channels</span><span class="p">)</span>
        <span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">relu</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">ReLU</span><span class="p">()</span>
        
        <span class="k">if</span> <span class="n">stride</span> <span class="o">!=</span> <span class="mi">1</span><span class="p">:</span>
            <span class="n">self</span><span class="p">.</span><span class="n">shortcut</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Sequential</span><span class="p">(</span>
                <span class="n">nn</span><span class="p">.</span><span class="nc">Conv2d</span><span class="p">(</span><span class="n">in_channels</span><span class="o">=</span><span class="n">input_channels</span><span class="p">,</span> <span class="n">out_channels</span><span class="o">=</span><span class="n">output_channels</span><span class="p">,</span> <span class="n">kernel_size</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="mi">0</span><span class="p">),</span>
                <span class="n">nn</span><span class="p">.</span><span class="nc">BatchNorm2d</span><span class="p">(</span><span class="n">output_channels</span><span class="p">)</span>
            <span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">self</span><span class="p">.</span><span class="n">shortcut</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Sequential</span><span class="p">()</span>
    
    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="n">z</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">block</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">z</span> <span class="o">+=</span> <span class="n">self</span><span class="p">.</span><span class="nf">shortcut</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">z</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">relu</span><span class="p">(</span><span class="n">z</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">z</span>
</code></pre></div></div>
<p>In the residual block (no bottleneck), there is the standard sequence of Conv2d-BatchNorm-ReLU. The key addition here is the addition of the shortcut connection.</p>

<p>When there is a stride of 2 in the block, there is a change in dimensionality (width and height are reduced by 2), so the dimensionality reduction is applied.</p>

<h2 id="bottleneck-residual-block">Bottleneck residual block</h2>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">ResidualBlockBottleneck</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="n">expansion</span> <span class="o">=</span> <span class="mi">4</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">input_channels</span><span class="p">,</span> <span class="n">in_channels</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="mi">1</span><span class="p">):</span>
        <span class="nf">super</span><span class="p">(</span><span class="n">ResidualBlockBottleneck</span><span class="p">,</span> <span class="n">self</span><span class="p">).</span><span class="nf">__init__</span><span class="p">()</span>

        <span class="n">self</span><span class="p">.</span><span class="n">block</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Sequential</span><span class="p">(</span>
            <span class="n">nn</span><span class="p">.</span><span class="nc">Conv2d</span><span class="p">(</span><span class="n">in_channels</span><span class="o">=</span><span class="n">input_channels</span><span class="p">,</span> <span class="n">out_channels</span><span class="o">=</span><span class="n">in_channels</span><span class="p">,</span> <span class="n">kernel_size</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="bp">False</span><span class="p">),</span>
            <span class="n">nn</span><span class="p">.</span><span class="nc">BatchNorm2d</span><span class="p">(</span><span class="n">in_channels</span><span class="p">),</span>
            <span class="n">nn</span><span class="p">.</span><span class="nc">ReLU</span><span class="p">(),</span>
            <span class="n">nn</span><span class="p">.</span><span class="nc">Conv2d</span><span class="p">(</span><span class="n">in_channels</span><span class="o">=</span><span class="n">in_channels</span><span class="p">,</span> <span class="n">out_channels</span><span class="o">=</span><span class="n">in_channels</span><span class="p">,</span> <span class="n">kernel_size</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="n">stride</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="bp">False</span><span class="p">),</span>
            <span class="n">nn</span><span class="p">.</span><span class="nc">BatchNorm2d</span><span class="p">(</span><span class="n">in_channels</span><span class="p">),</span>
            <span class="n">nn</span><span class="p">.</span><span class="nc">ReLU</span><span class="p">(),</span>
            <span class="n">nn</span><span class="p">.</span><span class="nc">Conv2d</span><span class="p">(</span><span class="n">in_channels</span><span class="o">=</span><span class="n">in_channels</span><span class="p">,</span> <span class="n">out_channels</span><span class="o">=</span><span class="n">in_channels</span><span class="o">*</span><span class="mi">4</span><span class="p">,</span> <span class="n">kernel_size</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="bp">False</span><span class="p">),</span>
            <span class="n">nn</span><span class="p">.</span><span class="nc">BatchNorm2d</span><span class="p">(</span><span class="n">in_channels</span><span class="o">*</span><span class="mi">4</span><span class="p">)</span>
        <span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">relu</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">ReLU</span><span class="p">()</span>
        
        <span class="k">if</span> <span class="n">stride</span> <span class="o">!=</span> <span class="mi">1</span> <span class="ow">or</span> <span class="n">input_channels</span> <span class="o">!=</span> <span class="n">self</span><span class="p">.</span><span class="n">expansion</span><span class="o">*</span><span class="n">in_channels</span><span class="p">:</span>
            <span class="n">self</span><span class="p">.</span><span class="n">shortcut</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Sequential</span><span class="p">(</span>
                <span class="n">nn</span><span class="p">.</span><span class="nc">Conv2d</span><span class="p">(</span><span class="n">in_channels</span><span class="o">=</span><span class="n">input_channels</span><span class="p">,</span> <span class="n">out_channels</span><span class="o">=</span><span class="n">in_channels</span><span class="o">*</span><span class="mi">4</span><span class="p">,</span> <span class="n">kernel_size</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="n">stride</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="bp">False</span><span class="p">),</span>
                <span class="n">nn</span><span class="p">.</span><span class="nc">BatchNorm2d</span><span class="p">(</span><span class="n">in_channels</span><span class="o">*</span><span class="mi">4</span><span class="p">)</span>
            <span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">self</span><span class="p">.</span><span class="n">shortcut</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Sequential</span><span class="p">()</span>
    
    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="n">z</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">block</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">z</span> <span class="o">+=</span> <span class="n">self</span><span class="p">.</span><span class="nf">shortcut</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">z</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">relu</span><span class="p">(</span><span class="n">z</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">z</span>
</code></pre></div></div>
<p>The new addition here is the 1 by 1 convolutions. Also, the code checks for <code class="language-plaintext highlighter-rouge">stride != 1</code> and <code class="language-plaintext highlighter-rouge">input_channels != self.expansion*in_channels</code>. The second condition checks if we are repeating the same residual block or if we are moving on to the next layer.</p>

<p>For example, for the first layer in ResNet50, the <code class="language-plaintext highlighter-rouge">input_channels</code> would be 64 as a result of preliminary layers. The <code class="language-plaintext highlighter-rouge">in_channels</code> is 64. The first layer repeats the bottleneck block 3 times. The first time, <code class="language-plaintext highlighter-rouge">input_channels</code> and <code class="language-plaintext highlighter-rouge">self.expansion*in_channels</code> would be 64 and \(4\times64=256\) respectively. Thus, there would be a dimensionality reduction. The second and third layers would then have <code class="language-plaintext highlighter-rouge">input_channels</code> and <code class="language-plaintext highlighter-rouge">self.expansion*in_channels</code> of 256, meaning no dimensionality reduction would be required.</p>

<p>With these blocks coded, making a full ResNet is as easy as stacking them on top of each other.</p>

<h2 id="conclusion">Conclusion</h2>
<p>The training accuracy of my VGG implementation plateued around 93.4%. My ResNet34 CIFAR10 variant managed to reach almost 95% before plateuing, suggesting that the skip connections managed to reduce training degradation as the paper set out to do. The CIFAR10 dataset is far smaller than ImageNet - probably why we do not see as big of a difference between VGG and ResNet.</p>

<p>Overall, this was a good paper to implement after VGG, due to small but significant alterations to the code. Next up: DenseNet.</p>]]></content><author><name></name></author><category term="computer-vision" /><category term="implementation" /><summary type="html"><![CDATA[Understanding and implementing Residual Networks]]></summary></entry><entry><title type="html">My first full paper implementation</title><link href="https://boosungkim.com/blog/2023/first-paper-implementation/" rel="alternate" type="text/html" title="My first full paper implementation" /><published>2023-05-31T19:53:00+00:00</published><updated>2023-05-31T19:53:00+00:00</updated><id>https://boosungkim.com/blog/2023/first-paper-implementation</id><content type="html" xml:base="https://boosungkim.com/blog/2023/first-paper-implementation/"><![CDATA[<p>Link to <a href="https://arxiv.org/abs/1409.1556">paper</a>
Link to <a href="https://github.com/boosungkim/milestone-cnn-model-implementations">my code</a></p>

<p>I have read several Deep Learning research papers at this point, but I have never fully implemented one from scratch. I decided to finally try with the VGG model from <a href="https://arxiv.org/abs/1409.1556">Very Deep Convolutional Networks for Large-Scale Image Recognition</a>.</p>

<h2 id="vgg-explained">VGG Explained</h2>
<p>The VGG paper was unique in that it proved the importance of depth in image classification. Unlike its predecessors, VGG utilizes simple filters and a repetitive structure. In return for its simplicity, VGG has a far greater depth, the deepest variation in the paper having 19 layers, which was a lot for its time.</p>

<h2 id="hurdles-along-the-way">Hurdles along the way</h2>
<p>Implementing VGG from scratch initially introduced a few challenges. While it may seem easy now, converting the architecture represented in the text into Pytorch code was confusing at first.</p>

<h3 id="1-coding-and-refactoring">1. Coding and refactoring</h3>
<p>At first, I manually wrote the model by simply writing out every single layer in the model class <code class="language-plaintext highlighter-rouge">__init__()</code> function. Obviously, this is not good code.</p>

<p>In the end, I referenced some code online to see how other people organized their code. Others did so by utilizing enumeration, <code class="language-plaintext highlighter-rouge">for</code> loops and helper functions.</p>

<h3 id="2-finding-hyperparameters">2. Finding hyperparameters</h3>
<p>Another trouble I ran into was finding the parameters and hyperparameters for the architecture in the code. Thankfully, the VGG paper has a very detailed architecture diagram.</p>

<p><img src="/assets/img/blogs/2023/2023-05-31-first-paper-implementation/vgg-architecture.png" alt="image" /></p>

<p>As can be seen above, the sizes and number of convolutional filters for each layer is quite apparent. However, for instance, I had to read the text to find out that the stride and padding are both 1 pixel.</p>

<p>Not difficult at all, as the paper is very detailed, but some of the more recent papers I tried recently are not as explicit.</p>

<h3 id="3-checking-the-model-structure">3. Checking the model structure</h3>
<p>Once I had a seemingly functioning model, I needed a way of making sure that recreated the VGG model exactly. There are two methods I used:</p>

<p>1) Passing a tensor through the model</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">t1</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">randn</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">3</span><span class="p">,</span><span class="mi">224</span><span class="p">,</span><span class="mi">224</span><span class="p">)</span>
</code></pre></div></div>
<blockquote>
  <p>A tensor representation of an image from ImageNet.</p>
</blockquote>

<p>By sending this through the model, I was able to check if the model returns the correct output dimensions.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="o">&gt;&gt;&gt;</span> <span class="nf">print</span><span class="p">(</span><span class="nf">testing</span><span class="p">(</span><span class="n">t1</span><span class="p">).</span><span class="nf">size</span><span class="p">())</span>
<span class="n">torch</span><span class="p">.</span><span class="nc">Size</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1000</span><span class="p">])</span>
</code></pre></div></div>

<p>2) Torchinfo</p>

<p>However, this is not enough to ensure that the model architecture is correct. So, I used a Python library called <a href="https://github.com/TylerYep/torchinfo">Torchinfo</a> to print out the entire architecture. I ran into a few issues with Torchinfo later on, but that is not relevant here.</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>==========================================================================================
Layer (type:depth-idx)                   Output Shape              Param #
==========================================================================================
VGGModel                                 [1, 1000]                 --
├─Sequential: 1-1                        [1, 64, 112, 112]         --
│    └─Conv2d: 2-1                       [1, 64, 224, 224]         1,792
│    └─ReLU: 2-2                         [1, 64, 224, 224]         --
│    └─Conv2d: 2-3                       [1, 64, 224, 224]         36,928
│    └─ReLU: 2-4                         [1, 64, 224, 224]         --
│    └─MaxPool2d: 2-5                    [1, 64, 112, 112]         --
├─Sequential: 1-2                        [1, 128, 56, 56]          --
│    └─Conv2d: 2-6                       [1, 128, 112, 112]        73,856
│    └─ReLU: 2-7                         [1, 128, 112, 112]        --
│    └─Conv2d: 2-8                       [1, 128, 112, 112]        147,584
│    └─ReLU: 2-9                         [1, 128, 112, 112]        --
│    └─MaxPool2d: 2-10                   [1, 128, 56, 56]          --
├─Sequential: 1-3                        [1, 256, 28, 28]          --
│    └─Conv2d: 2-11                      [1, 256, 56, 56]          295,168
│    └─ReLU: 2-12                        [1, 256, 56, 56]          --
│    └─Conv2d: 2-13                      [1, 256, 56, 56]          590,080
│    └─ReLU: 2-14                        [1, 256, 56, 56]          --
│    └─Conv2d: 2-15                      [1, 256, 56, 56]          590,080
│    └─ReLU: 2-16                        [1, 256, 56, 56]          --
│    └─Conv2d: 2-17                      [1, 256, 56, 56]          590,080
│    └─ReLU: 2-18                        [1, 256, 56, 56]          --
│    └─MaxPool2d: 2-19                   [1, 256, 28, 28]          --
├─Sequential: 1-4                        [1, 512, 14, 14]          --
│    └─Conv2d: 2-20                      [1, 512, 28, 28]          1,180,160
│    └─ReLU: 2-21                        [1, 512, 28, 28]          --
│    └─Conv2d: 2-22                      [1, 512, 28, 28]          2,359,808
│    └─ReLU: 2-23                        [1, 512, 28, 28]          --
│    └─Conv2d: 2-24                      [1, 512, 28, 28]          2,359,808
│    └─ReLU: 2-25                        [1, 512, 28, 28]          --
│    └─Conv2d: 2-26                      [1, 512, 28, 28]          2,359,808
│    └─ReLU: 2-27                        [1, 512, 28, 28]          --
│    └─MaxPool2d: 2-28                   [1, 512, 14, 14]          --
├─Sequential: 1-5                        [1, 512, 7, 7]            --
│    └─Conv2d: 2-29                      [1, 512, 14, 14]          2,359,808
│    └─ReLU: 2-30                        [1, 512, 14, 14]          --
│    └─Conv2d: 2-31                      [1, 512, 14, 14]          2,359,808
│    └─ReLU: 2-32                        [1, 512, 14, 14]          --
│    └─Conv2d: 2-33                      [1, 512, 14, 14]          2,359,808
│    └─ReLU: 2-34                        [1, 512, 14, 14]          --
│    └─Conv2d: 2-35                      [1, 512, 14, 14]          2,359,808
│    └─ReLU: 2-36                        [1, 512, 14, 14]          --
│    └─MaxPool2d: 2-37                   [1, 512, 7, 7]            --
├─Flatten: 1-6                           [1, 25088]                --
├─Sequential: 1-7                        [1, 1000]                 --
│    └─Linear: 2-38                      [1, 4096]                 102,764,544
│    └─ReLU: 2-39                        [1, 4096]                 --
│    └─Linear: 2-40                      [1, 4096]                 16,781,312
│    └─ReLU: 2-41                        [1, 4096]                 --
│    └─Linear: 2-42                      [1, 1000]                 4,097,000
│    └─Softmax: 2-44                     [1, 1000]                 --
==========================================================================================
Total params: 143,667,240
Trainable params: 143,667,240
Non-trainable params: 0
Total mult-adds (Units.GIGABYTES): 19.65
==========================================================================================
Input size (MB): 0.60
Forward/backward pass size (MB): 118.89
Params size (MB): 574.67
Estimated Total Size (MB): 694.16
==========================================================================================
</code></pre></div></div>

<p>The example above is my implementation of the VGG19 model. The structure seems to match up with the paper’s VGG19. Additionally, the number of parameters of my custom model is roughly equivalent to that of VGG19 (144M parameters)!</p>

<p><img src="/assets/img/blogs/2023/2023-05-31-first-paper-implementation/vgg-param.png" alt="image" /></p>

<p>On a side note, you may wonder why there is no huge difference between the number of parameters in VGG11 (133M) and that of VGG19 (144M). The reason for that is that most of the parameters are located in the Fully Connected layers at the end of the each model. The number of parameters in the Convolutional layers pales in comparison to the number in Fully Connected layers, which is why, even though there is an 8-layer difference between VGG11 and VGG19, the numbers of parameters do not differ that much.</p>

<h3 id="4-the-dreaded-10-accuracy">4. The dreaded 10% accuracy</h3>
<p>When I ran this code on the CIFAR10 dataset, however, I got a 0.1 accuracy! That’s basically the same as randomly guessing (since there are 10 possible guesses). No matter how many times I tweaked the hyperparameters, I got the same 0.1.</p>

<p>The VGG model was created to be used on the ImageNet, which contains images of <code class="language-plaintext highlighter-rouge">224x224x3</code> dimensions. On the other hand, CIFAR10 contains images of <code class="language-plaintext highlighter-rouge">32x32x3</code> dimensions. The VGG is simple just too deep to learn the CIFAR10 dataset properly.</p>

<p>I resolved this issue by adding Batch Normalization to every block in the architecture. Since Batch Normalization ensures that the activations of each layer have zero mean and unit variance, the gradients are well scaled throughout the network. I improved the performance even further by removing the final few Fully Connected layers.</p>

<p>In the end, with even more hyperparameter tweaking, I was left with a training accuracy of 93.4%, a validation accuracy of 88.7%, and a testing accuracy of 88.2%.</p>

<h2 id="conclusion">Conclusion</h2>
<p>As someone who had never fully implemented an entire paper before, I struggled with some of the code conversion in the beginning. However, compared to models like ResNet and DenseNet, VGG definitely was the easiest to implement.</p>

<p>I can see why people often recommend beginners to start by implementing papers despite the difficulty - there really is no better way to learn.</p>

<p>What I learnt:</p>
<ul>
  <li>NN model debugging skills</li>
  <li>Analyzing parameters and hyperparameters from papers</li>
  <li>Adjusting the model to fit the dataset</li>
</ul>]]></content><author><name></name></author><category term="computer-vision" /><category term="implementation" /><summary type="html"><![CDATA[Implementing the VGG paper from "Very Deep Convolutional Networks for Large-Scale Image Recognition"]]></summary></entry></feed>