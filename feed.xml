<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.3.2">Jekyll</generator><link href="https://boosungkim.com/feed.xml" rel="self" type="application/atom+xml" /><link href="https://boosungkim.com/" rel="alternate" type="text/html" hreflang="en" /><updated>2023-06-05T06:02:35+00:00</updated><id>https://boosungkim.com/feed.xml</id><title type="html">blank</title><subtitle>A lifelong learner interested in the modern world.
</subtitle><entry><title type="html">My first full paper implementation</title><link href="https://boosungkim.com/blog/2023/first-paper-implementation/" rel="alternate" type="text/html" title="My first full paper implementation" /><published>2023-05-31T19:53:00+00:00</published><updated>2023-05-31T19:53:00+00:00</updated><id>https://boosungkim.com/blog/2023/first-paper-implementation</id><content type="html" xml:base="https://boosungkim.com/blog/2023/first-paper-implementation/"><![CDATA[<p>Link to <a href="https://arxiv.org/abs/1409.1556">paper</a>
Link to <a href="https://github.com/boosungkim/milestone-cnn-model-implementations">my code</a></p>

<p>I have read several Deep Learning research papers at this point, but I have never fully implemented one from scratch. I decided to finally try with the VGG model from <a href="https://arxiv.org/abs/1409.1556">Very Deep Convolutional Networks for Large-Scale Image Recognition</a>.</p>

<h2 id="vgg-explained">VGG Explained</h2>
<p>The VGG paper was unique in that it proved the importance of depth in image classification. Unlike its predecessors, VGG utilizes simple filters and a repetitive structure. In return for its simplicity, VGG has a far greater depth, the deepest variation in the paper having 19 layers, which was a lot for its time.</p>

<h2 id="hurdles-along-the-way">Hurdles along the way</h2>
<p>Implementing VGG from scratch initially introduced a few challenges. While it may seem easy now, converting the architecture represented in the text into Pytorch code was confusing at first.</p>

<h3 id="1-coding-and-refactoring">1. Coding and refactoring</h3>
<p>At first, I manually wrote the model by simply writing out every single layer in the model class <code class="language-plaintext highlighter-rouge">__init__()</code> function. Obviously, this is not good code.</p>

<p>In the end, I referenced some code online to see how other people organized their code. Others did so by utilizing enumeration, <code class="language-plaintext highlighter-rouge">for</code> loops and helper functions.</p>

<h3 id="2-finding-hyperparameters">2. Finding hyperparameters</h3>
<p>Another trouble I ran into was finding the parameters and hyperparameters for the architecture in the code. Thankfully, the VGG paper has a very detailed architecture diagram.</p>

<p><img src="/assets/img/blogs/2023-05-31-first-paper-implementation/vgg-architecture.png" alt="image" /></p>

<p>As can be seen above, the sizes and number of convolutional filters for each layer is quite apparent. However, for instance, I had to read the text to find out that the stride and padding are both 1 pixel.</p>

<p>Not difficult at all, as the paper is very detailed, but some of the more recent papers I tried recently are not as explicit.</p>

<h3 id="3-checking-the-model-structure">3. Checking the model structure</h3>
<p>Once I had a seemingly functioning model, I needed a way of making sure that recreated the VGG model exactly. There are two methods I used:</p>

<p>1) Passing a tensor through the model</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">t1</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">randn</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">3</span><span class="p">,</span><span class="mi">224</span><span class="p">,</span><span class="mi">224</span><span class="p">)</span>
</code></pre></div></div>
<blockquote>
  <p>A tensor representation of an image from ImageNet.</p>
</blockquote>

<p>By sending this through the model, I was able to check if the model returns the correct output dimensions.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="o">&gt;&gt;&gt;</span> <span class="nf">print</span><span class="p">(</span><span class="nf">testing</span><span class="p">(</span><span class="n">t1</span><span class="p">).</span><span class="nf">size</span><span class="p">())</span>
<span class="n">torch</span><span class="p">.</span><span class="nc">Size</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1000</span><span class="p">])</span>
</code></pre></div></div>

<p>2) Torchinfo</p>

<p>However, this is not enough to ensure that the model architecture is correct. So, I used a Python library called <a href="https://github.com/TylerYep/torchinfo">Torchinfo</a> to print out the entire architecture. I ran into a few issues with Torchinfo later on, but that is not relevant here.</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>==========================================================================================
Layer (type:depth-idx)                   Output Shape              Param #
==========================================================================================
VGGModel                                 [1, 1000]                 --
├─Sequential: 1-1                        [1, 64, 112, 112]         --
│    └─Conv2d: 2-1                       [1, 64, 224, 224]         1,792
│    └─ReLU: 2-2                         [1, 64, 224, 224]         --
│    └─Conv2d: 2-3                       [1, 64, 224, 224]         36,928
│    └─ReLU: 2-4                         [1, 64, 224, 224]         --
│    └─MaxPool2d: 2-5                    [1, 64, 112, 112]         --
├─Sequential: 1-2                        [1, 128, 56, 56]          --
│    └─Conv2d: 2-6                       [1, 128, 112, 112]        73,856
│    └─ReLU: 2-7                         [1, 128, 112, 112]        --
│    └─Conv2d: 2-8                       [1, 128, 112, 112]        147,584
│    └─ReLU: 2-9                         [1, 128, 112, 112]        --
│    └─MaxPool2d: 2-10                   [1, 128, 56, 56]          --
├─Sequential: 1-3                        [1, 256, 28, 28]          --
│    └─Conv2d: 2-11                      [1, 256, 56, 56]          295,168
│    └─ReLU: 2-12                        [1, 256, 56, 56]          --
│    └─Conv2d: 2-13                      [1, 256, 56, 56]          590,080
│    └─ReLU: 2-14                        [1, 256, 56, 56]          --
│    └─Conv2d: 2-15                      [1, 256, 56, 56]          590,080
│    └─ReLU: 2-16                        [1, 256, 56, 56]          --
│    └─Conv2d: 2-17                      [1, 256, 56, 56]          590,080
│    └─ReLU: 2-18                        [1, 256, 56, 56]          --
│    └─MaxPool2d: 2-19                   [1, 256, 28, 28]          --
├─Sequential: 1-4                        [1, 512, 14, 14]          --
│    └─Conv2d: 2-20                      [1, 512, 28, 28]          1,180,160
│    └─ReLU: 2-21                        [1, 512, 28, 28]          --
│    └─Conv2d: 2-22                      [1, 512, 28, 28]          2,359,808
│    └─ReLU: 2-23                        [1, 512, 28, 28]          --
│    └─Conv2d: 2-24                      [1, 512, 28, 28]          2,359,808
│    └─ReLU: 2-25                        [1, 512, 28, 28]          --
│    └─Conv2d: 2-26                      [1, 512, 28, 28]          2,359,808
│    └─ReLU: 2-27                        [1, 512, 28, 28]          --
│    └─MaxPool2d: 2-28                   [1, 512, 14, 14]          --
├─Sequential: 1-5                        [1, 512, 7, 7]            --
│    └─Conv2d: 2-29                      [1, 512, 14, 14]          2,359,808
│    └─ReLU: 2-30                        [1, 512, 14, 14]          --
│    └─Conv2d: 2-31                      [1, 512, 14, 14]          2,359,808
│    └─ReLU: 2-32                        [1, 512, 14, 14]          --
│    └─Conv2d: 2-33                      [1, 512, 14, 14]          2,359,808
│    └─ReLU: 2-34                        [1, 512, 14, 14]          --
│    └─Conv2d: 2-35                      [1, 512, 14, 14]          2,359,808
│    └─ReLU: 2-36                        [1, 512, 14, 14]          --
│    └─MaxPool2d: 2-37                   [1, 512, 7, 7]            --
├─Flatten: 1-6                           [1, 25088]                --
├─Sequential: 1-7                        [1, 1000]                 --
│    └─Linear: 2-38                      [1, 4096]                 102,764,544
│    └─ReLU: 2-39                        [1, 4096]                 --
│    └─Linear: 2-40                      [1, 4096]                 16,781,312
│    └─ReLU: 2-41                        [1, 4096]                 --
│    └─Linear: 2-42                      [1, 1000]                 4,097,000
│    └─Softmax: 2-44                     [1, 1000]                 --
==========================================================================================
Total params: 143,667,240
Trainable params: 143,667,240
Non-trainable params: 0
Total mult-adds (Units.GIGABYTES): 19.65
==========================================================================================
Input size (MB): 0.60
Forward/backward pass size (MB): 118.89
Params size (MB): 574.67
Estimated Total Size (MB): 694.16
==========================================================================================
</code></pre></div></div>

<p>The example above is my implementation of the VGG19 model. The structure seems to match up with the paper’s VGG19. Additionally, the number of parameters of my custom model is roughly equivalent to that of VGG19 (144M parameters)!</p>

<p><img src="/assets/img/blogs/2023-05-31-first-paper-implementation/vgg-param.png" alt="image" /></p>

<p>On a side note, you may wonder why there is no huge difference between the number of parameters in VGG11 (133M) and that of VGG19 (144M). The reason for that is that most of the parameters are located in the Fully Connected layers at the end of the each model. The number of parameters in the Convolutional layers pales in comparison to the number in Fully Connected layers, which is why, even though there is an 8-layer difference between VGG11 and VGG19, the numbers of parameters do not differ that much.</p>

<h3 id="4-the-dreaded-10-accuracy">4. The dreaded 10% accuracy</h3>
<p>When I ran this code on the CIFAR10 dataset, however, I got a 0.1 accuracy! That’s basically the same as randomly guessing (since there are 10 possible guesses). No matter how many times I tweaked the hyperparameters, I got the same 0.1.</p>

<p>The VGG model was created to be used on the ImageNet, which contains images of <code class="language-plaintext highlighter-rouge">224x224x3</code> dimensions. On the other hand, CIFAR10 contains images of <code class="language-plaintext highlighter-rouge">32x32x3</code> dimensions. The VGG is simple just too deep to learn the CIFAR10 dataset properly.</p>

<p>I resolved this issue by adding Batch Normalization to every block in the architecture. Since Batch Normalization ensures that the activations of each layer have zero mean and unit variance, the gradients are well scaled throughout the network. I improved the performance even further by removing the final few Fully Connected layers.</p>

<p>In the end, with even more hyperparameter tweaking, I was left with a training accuracy of 95%, a validation accuracy of 88.7%, and a testing accuracy of 88.2%.</p>

<h2 id="conclusion">Conclusion</h2>
<p>As someone who had never fully implemented an entire paper before, I struggled with some of the code conversion in the beginning. However, compared to models like ResNet and DenseNet, VGG definitely was the easiest to implement.</p>

<p>I can see why people often recommend beginners to start by implementing papers despite the difficulty - there really is no better way to learn.</p>

<p>What I learnt:</p>
<ul>
  <li>NN model debugging skills</li>
  <li>Analyzing parameters and hyperparameters from papers</li>
  <li>Adjusting the model to fit the dataset</li>
</ul>]]></content><author><name></name></author><category term="computer-vision" /><category term="implementation" /><summary type="html"><![CDATA[Implementing the VGG paper from "Very Deep Convolutional Networks for Large-Scale Image Recognition"]]></summary></entry><entry><title type="html">Sticky business of setting up a web server</title><link href="https://boosungkim.com/blog/2020/sticky-business-of-setting-up-a-web-server/" rel="alternate" type="text/html" title="Sticky business of setting up a web server" /><published>2020-12-27T00:00:00+00:00</published><updated>2020-12-27T00:00:00+00:00</updated><id>https://boosungkim.com/blog/2020/sticky-business-of-setting-up-a-web-server</id><content type="html" xml:base="https://boosungkim.com/blog/2020/sticky-business-of-setting-up-a-web-server/"><![CDATA[<h1 id="0-introduction">0: Introduction</h1>
<p>Earlier this week, I finally finished setting up my web server and decided to write this “documentation.” Looking back now, these steps may seem evident, but it was a nightmare when I started up due my complete lack of prior experience. I hope that this will be of use, whether it be to my future self setting up another server or to someone else starting from scratch.</p>

<p>I used NameCheap for the domain name, Vultr for a VPS, CertBot for SSL, CloudFlare for the DNS servers, and the classic LAMP (Linux Apache MariaDB PHP) stack to host my web server.</p>

<p>I only mention the major steps in chronological order to serve as a general overview of creating a web server from scratch.</p>

<h1 id="1-basic-html-and-css">1: Basic HTML and CSS</h1>
<p>Before anything else, I first had to write the foundations of my website in HTML and CSS. I found the Interneting is Hard tutorial to be the most intuitive.</p>

<h1 id="2-purchasing-a-vps-and-a-domain">2: Purchasing a VPS and a domain</h1>
<p>Purchasing a domain name and a VPS was easy enough. I decided to go with the popular choice of (first name)(last name).com. After I obtained a VPS, I set the A NAME for my root domain boosungkim.com and the CNAME for my subdomain www.boosungkim.com. This way, people will be directed to my site regardless of their URL choice. At this point, I was using the default NameCheap DNS servers.</p>

<h1 id="3-setting-a-lamp-stack">3: Setting a LAMP stack</h1>
<p>I believe that this is probably the section that took the second longest time to figure out. After I set up Debian on the VPS and installed Apache2, I still had no idea how to configure the Apache settings. I ended up jumping back and forth among several documentations and guides, but I mostly used these two: <a href="linux.com">Linux.com</a> and <a href="apache.org">apache.org</a>. The guide only shows the basic configuration of publishing an HTTP website, so I had to make alterations later on.</p>

<h1 id="4-file-uploading-with-rsync-and-ssh">4: File uploading with rsync and SSH</h1>
<p>Up to this point, it seemed like everyone was using some sort of graphical interface to upload their files. Since I did not have access to any, I originally considered pushing and pulling via GitHub. Thankfully, I figured out a simpler solution: I used the Linux command rsync over SSH. It was direct and fast.</p>

<h1 id="5-ssl-https">5: SSL (HTTPS)</h1>
<p>Now this is the part I got stuck on for the longest period of time, which was quite surprising. The vast majority, if not all, of the tutorials state that CertBot is simple to use. It certainly is easy to download, but the configuration process was convoluted for me as a beginner.</p>

<p>At first, I made the mistake of getting the default certbot, which is designed for websites with only one domain. In the earlier part of this documentation, I mentioned that I had a subdomain for www.boosungkim.com. The subdomain and the default certbot conflicted, and I ended up completely destroying my web server. I had to reset the VPS and start all over again.</p>

<p>After the restart, I realized that I need to use the wilcard certbot to install HTTPS for all the subdomains as well as the main one. I then immediately faced the issue of DNS plugins—NameCheap does not support certbot, as they have their own SSL service (which is charged). A comment on a <a href="https://old.reddit.com/r/homelab/comments/84no8z/lets_encrypt_certbot_namecheap_and_tlssni_or_dns/dvqz8pz/">forum</a> inspired me to switch to the CloudFlare DNS servers and use their plugin to install HTTPS.</p>

<p>After the successful installation, I had to reconfigure my VirtualHost file to change from the HTTP port (80) to the HTTPS port (443). I followed the following <a href="https://cwiki.apache.org/confluence/display/HTTPD/ExampleVhosts">guide</a>.</p>

<p>While these may seem like straightforward issues with effortless solutions, I can promise you that when I had no idea what was going on, understanding the core complication was like finding a needle in a haystack, except that the needle thankfully left a trail of clues here and there.</p>

<h1 id="6-rss-and-the-email-subscription-system">6: RSS and the Email subscription system</h1>
<p>Creating the RSS for my blog was quite a relief, as it was near trivial compared to the SSL mess right before this step. I used the documentations <a href="https://www.w3schools.com/xml/xml_rss.asp">w3schools</a> and <a href="https://validator.w3.org/feed/docs/rss2.html">FEED validator</a>, and you can view my XML file on my website as well.</p>

<p>I decided to use PHPList for the email subscription system, because it uses PHP and mariaDB. I installed an SMTP client on my server, then the PHPList software. While the official <a href="https://www.phplist.org/manual/books/phplist-manual/page/installing-phplist-manually">PHPList documentation</a> includes a part of the installation process, it skims over the configuration of the database. I would recommend this resource here: <a href="https://www.rosehosting.com/blog/how-to-install-phplist-on-a-centos-7-vps/">Rosehosting</a>.</p>

<h1 id="7-completion">7: Completion?</h1>
<p>And that was the final major course of action I had to take. The current version of my website, “version 1” as I like to call it, was completed this Wednesday (2020/07/01).</p>

<p>As mentioned, I did not include all the minor difficulties I had, such as granting different files various permissions, creating a contact form, and configuring the php file, but most of those were resolved by reading the documentation (often for hours).</p>

<h1 id="learning-how-to-troubleshoot">Learning how to troubleshoot</h1>
<p>Perhaps this is a stretch, but I believe that I learnt the basics of troubleshooting. To look up a problem, I had to narrow down what the scope of the issue. No search results came up when I simply said that my website sometimes showed up an error page. I had to specify the error messages and try to derive the main issue from them. Did I make a mistake in the Apache configuration file, or is port 443 closed? Perhaps it is the firewall? Oh no, I simply misdirected the website root folder.</p>

<h1 id="the-future">The future</h1>
<p>As mentioned, I consider the current website only Version 1.0. There are many more changes I would like to make over the next few months and even years. For one, I would like to automate the blog posting process. Right now, I have to code 5 times every time I post: the standalone blog file, the blog index page, the blog rolling page, the XML file, and the email. It would be far more convenient to have a single script that would code all that for me.</p>

<p>But I leave my website where it is for now, as I would like to work on other projects before the summer ends.</p>

<p>I hope this helps someone and thanks for reading!</p>

<p><a href="https://github.com/boosungkim/original-website">The front end code</a></p>]]></content><author><name></name></author><category term="misc" /><summary type="html"><![CDATA[the process of setting up my original website]]></summary></entry><entry><title type="html">Blog 0 - Why I started a blog</title><link href="https://boosungkim.com/blog/2020/why-i-started-a-blog/" rel="alternate" type="text/html" title="Blog 0 - Why I started a blog" /><published>2020-12-26T00:00:00+00:00</published><updated>2020-12-26T00:00:00+00:00</updated><id>https://boosungkim.com/blog/2020/why-i-started-a-blog</id><content type="html" xml:base="https://boosungkim.com/blog/2020/why-i-started-a-blog/"><![CDATA[<p>So this is my first official blog post of this website! I have to admit that setting up the basics of this web server was a lot more complicated than I anticipated, but here we are!</p>

<p>On this site, I plan to post mainly about Math and Computer Science, my two great passions and also my majors. I will upload my opinions on various technologies, documentations I write while working on personal projects, and even potentially devlogs for particular projects.</p>

<p>The primary reason I started writing about the my Math and Computer Science endeavors is to practice my communication skills. Up till now, I have worked on my technology skillsets, but not so much on communication. I realized recently that I should be able to convey my ideas in a clear and concise manner—even to people who may not be familiar with technologies I work with. Like most things, the best way to better my interpersonal skills is to practice.</p>

<p>While that may be my main reason, I am already learning far beyond just communication skills: I learnt how to set up a server, the basics of the LAMP stack, and more. More on that on the next blog post about setting up a web server.</p>]]></content><author><name></name></author><category term="misc" /><summary type="html"><![CDATA[my motivation for starting a blog]]></summary></entry></feed>